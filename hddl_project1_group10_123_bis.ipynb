{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b8887d3",
   "metadata": {},
   "source": [
    "# Project n°1 - Cats or dogs?\n",
    "\n",
    "Authors : Selim Ben Abdallah, Paola De Truchis, Eduardo De Jesus Zancanaro Garcia and Edda Iveland"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7856a448",
   "metadata": {},
   "source": [
    "* The goal of this project is to study, classify and segment the images in the `Oxford-IIIT Pet` dataset.\n",
    "* This dataset contains images and lables of cats and dogs of 37 different races. \n",
    "\n",
    "\n",
    "We will start by loading the data and performing an exploratory analysis before implementing deep learning methods that will classify images into dogs or cats and the different breeds. Lastly, we will apply a U-net network to segment the images: specifying where in the image the cat or dog is situated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2478cf73",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4057e3",
   "metadata": {},
   "source": [
    "### Plan : \n",
    "\n",
    "* 0 - Libraries, util functions, global variables, ...\n",
    "* 1 - Exploratory Data Analysis\n",
    "* 2 - Binary Classification\n",
    "* 3 - Multiclass Classification\n",
    "* 4 - Segmentation\n",
    "* 5 - Comparision, Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82c615",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33423cae",
   "metadata": {},
   "source": [
    "## 0 - Libraries, data, util functions, variables, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2d9c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TRAINVAL_TXT = \"dataset/annotations/trainval.txt\"\n",
    "PATH_TO_TEST_TXT = \"dataset/annotations/test.txt\"\n",
    "PATH_TO_LIST_TXT = \"dataset/annotations/list.txt\"\n",
    "\n",
    "PATH_TO_IMG = \"dataset/images/\" # .jpg\n",
    "PATH_TO_TRIMAPS = \"dataset/annotations/trimaps/\" #.png\n",
    "PATH_TO_XML = \"dataset/annotations/xmls/\" #.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60adec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, classification_report, roc_curve, auc, average_precision_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075072e8",
   "metadata": {},
   "source": [
    "## 1 - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc1343f",
   "metadata": {},
   "source": [
    "### DataFrame construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a90407d",
   "metadata": {},
   "source": [
    "- The `dataset` contains two subfolders, `dataset/annotations/` and `dataset/images/`\n",
    "- Let's first create a DataFrame with all the information we can collet from these subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51599413",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "with open(PATH_TO_LIST_TXT, 'r') as f:\n",
    "    # we skip the first 6 lines (header)\n",
    "    for _ in range(6):\n",
    "        next(f)\n",
    "\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        # we skip empty lines if any\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        class_id, id_, species, breed_id = line.split()\n",
    "        breed_name = \"_\".join(class_id.split(\"_\")[:-1]) # breed name\n",
    "\n",
    "        data.append({\n",
    "            \"class_id\": class_id,\n",
    "            \"id\": int(id_) - 1, # 0-indexed\n",
    "            \"species\": int(species) - 1, # 0: cat, 1: dog\n",
    "            \"breed_id\": int(breed_id) - 1, # 0-24:Cat 0:11:Dog\n",
    "            \"breed_name\": breed_name,\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df[\"species_name\"] = df[\"species\"].map({0: \"Cat\", 1: \"Dog\"})\n",
    "df[\"img_path\"] = df[\"class_id\"].apply(lambda x: f\"{PATH_TO_IMG}{x}.jpg\")\n",
    "df[\"trimap_path\"] = df[\"class_id\"].apply(lambda x: f\"{PATH_TO_TRIMAPS}{x}.png\")\n",
    "df = df.set_index(\"class_id\", drop=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302df24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_TO_TRAINVAL_TXT, 'r') as f:\n",
    "    trainval_ids = set()\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        class_id, id_, species, breed_id = line.split()\n",
    "        trainval_ids.add(class_id)\n",
    "\n",
    "with open(PATH_TO_TEST_TXT, 'r') as f:\n",
    "    test_ids = set()\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        class_id, id_, species, breed_id = line.split()\n",
    "        test_ids.add(class_id)\n",
    "\n",
    "df[\"default_split\"] = \"unassigned\"\n",
    "df.loc[df[\"class_id\"].isin(trainval_ids), \"default_split\"] = \"trainval\"\n",
    "df.loc[df[\"class_id\"].isin(test_ids), \"default_split\"] = \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487b5e09",
   "metadata": {},
   "source": [
    "- We have extracted information from `dataset/images`, `dataset/annotations/trimaps`, we can complete our DataFrame with the data from the `_.xml` files in `dataset/annotations/xmls`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69bdd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(class_id):\n",
    "    path = Path(PATH_TO_XML) / f\"{class_id}.xml\"\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "\n",
    "    root = ET.parse(path).getroot()\n",
    "\n",
    "    size = root.find(\"size\")\n",
    "    obj = root.find(\"object\")\n",
    "    box = obj.find(\"bndbox\")\n",
    "\n",
    "    return {\n",
    "        \"width\":     int(size.find(\"width\").text),\n",
    "        \"height\":    int(size.find(\"height\").text),\n",
    "        \"depth\":     int(size.find(\"depth\").text),\n",
    "        \"segmented\": int(root.find(\"segmented\").text),\n",
    "        #\"xml_species\": obj.find(\"name\").text, we already have species info\n",
    "        \"pose\":        obj.find(\"pose\").text,\n",
    "        \"truncated\":   int(obj.find(\"truncated\").text),\n",
    "        \"occluded\":    int(obj.find(\"occluded\").text),\n",
    "        \"difficult\":   int(obj.find(\"difficult\").text),\n",
    "        \"bb_xmin\": int(box.find(\"xmin\").text),\n",
    "        \"bb_ymin\": int(box.find(\"ymin\").text),\n",
    "        \"bb_xmax\": int(box.find(\"xmax\").text),\n",
    "        \"bb_ymax\": int(box.find(\"ymax\").text),\n",
    "        \"xml_path\": str(path),\n",
    "    }\n",
    "\n",
    "\n",
    "df = df.set_index(\"class_id\", drop=False)\n",
    "\n",
    "for col in [\n",
    "    \"width\",\"height\",\"depth\",\"segmented\",\"pose\",\n",
    "    \"truncated\",\"occluded\",\"difficult\",\"bb_xmin\",\"bb_ymin\",\"bb_xmax\",\"bb_ymax\"\n",
    "]:\n",
    "    df[col] = pd.NA\n",
    "\n",
    "for cid in df.index:\n",
    "    for k, v in parse_xml(cid).items():\n",
    "        df.at[cid, k] = v\n",
    "\n",
    "col_1 = df.pop(\"img_path\")\n",
    "col_2 = df.pop(\"trimap_path\")\n",
    "df.insert(len(df.columns), \"img_path\", col_1)\n",
    "df.insert(len(df.columns), \"trimap_path\", col_2)\n",
    "\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805f6a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Are all rows unique ?\", not df.duplicated().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9897a55b",
   "metadata": {},
   "source": [
    "- All rows are unique, no duplicates in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8381c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files(path_to_img, path_to_xml, path_to_trimaps):\n",
    "    trimaps_count = len(list(Path(path_to_trimaps).glob(\"*.png\")))\n",
    "    img_count = len(list(Path(path_to_img).glob(\"*.jpg\")))\n",
    "    xml_count = len(list(Path(path_to_xml).glob(\"*.xml\")))\n",
    "    return img_count, xml_count, trimaps_count\n",
    "\n",
    "img_count, xml_count, trimaps_count = count_files(PATH_TO_IMG, PATH_TO_XML, PATH_TO_TRIMAPS)\n",
    "print(\"Number of PNG files (masks):\", trimaps_count)\n",
    "print(\"Number of JPG files (images):\", img_count)\n",
    "print(\"Number of XML files (additional info):\", xml_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453a5d27",
   "metadata": {},
   "source": [
    "- There are 7390 images, associated to 7390 segmentation masks\n",
    "- However the data from the .xml files (bounding box, pose, ...) is missing for 3704 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b29b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec92345",
   "metadata": {},
   "source": [
    "### Train / Val / Test distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b030a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"default_split\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc960dd2",
   "metadata": {},
   "source": [
    "- The default trainval / test split is balanced in terms of size, but not ideal for deep learning\n",
    "- The default split has not a proper validation set, so as is, we can't monitor overfitting, learning curves, hyperparemeters tuning.\n",
    "- Also, the defualt split is not identical per breed, is not stratified, and imbalanced for some breeds (-11 image imbalance for the stafford_bull_terrier breed between the default trainval and test splits, -8 image imbalance for the Bombay breed between the trainval and test splits)\n",
    "- The default test set is to large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fea48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df[\"breed_name\"], df[\"default_split\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fac48f",
   "metadata": {},
   "source": [
    "- We are going to re-split our dataset (80% : train, 10% val, 10% test), such that we can make it more efficient for our incoming deep learning tasks, and to be able to monitor the risk of overfitting, hyperparameter tunning and losses.\n",
    "- We will make sure the new split is stratified, so that each breeds are well represented in each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4921d0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first split into train+val and test\n",
    "train_ids, test_ids = train_test_split(\n",
    "    df.index,\n",
    "    test_size=0.1,\n",
    "    stratify=df[\"id\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# we then split train into train and val\n",
    "train_ids, val_ids = train_test_split(\n",
    "    train_ids,\n",
    "    test_size=0.1 / 0.9,  # ensures final 80/10/10\n",
    "    stratify=df.loc[train_ids, \"id\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df[\"new_split\"] = \"train\"\n",
    "df.loc[val_ids, \"new_split\"] = \"val\"\n",
    "df.loc[test_ids, \"new_split\"] = \"test\"\n",
    "\n",
    "breed_crosstab = pd.crosstab(df[\"breed_name\"], df[\"new_split\"])\n",
    "breed_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_per_breed = breed_crosstab.div(breed_crosstab.sum(axis=1), axis=0)\n",
    "ideal_breed_proportions = pd.Series({\"train\": 0.80, \"val\": 0.10, \"test\": 0.10})\n",
    "variance_relative_to_ideal_prop = ((proportion_per_breed - ideal_breed_proportions) ** 2).mean(axis=1)\n",
    "\n",
    "summary_proportion_per_breed = pd.DataFrame({\n",
    "    \"train_prop\": proportion_per_breed[\"train\"],\n",
    "    \"val_prop\": proportion_per_breed[\"val\"],\n",
    "    \"test_prop\": proportion_per_breed[\"test\"],\n",
    "    \"variance_from_ideal\": variance_relative_to_ideal_prop,\n",
    "})\n",
    "\n",
    "summary_proportion_per_breed.sort_values(\"variance_from_ideal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a23284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(proportion_per_breed, annot=True, fmt=\".3f\", cmap=\"Blues\")\n",
    "plt.title(\"Breed × Split — Normalized Proportions\")\n",
    "plt.xlabel(\"Split\")\n",
    "plt.ylabel(\"Breed\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb19c202",
   "metadata": {},
   "source": [
    "- With this new data split, we have roughly 80% of each breed in the train set, 10% in the val set, and 10% in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c616299",
   "metadata": {},
   "source": [
    "### Breed & Species distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c8217b",
   "metadata": {},
   "source": [
    "#### Dog vs Cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d0703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Number of Animals :\", df.shape[0])\n",
    "print(\"Number of Dogs :\", df[df[\"species_name\"] == \"Dog\"].shape[0])\n",
    "print(\"Number of Cats :\", df[df[\"species_name\"] == \"Cat\"].shape[0])\n",
    "\n",
    "df[\"species_name\"].value_counts().plot(\n",
    "    kind=\"pie\",\n",
    "    autopct=\"%1.1f%%\",\n",
    "    figsize=(7,7),\n",
    "    colors = [\"#A7D8F0\", \"#F5E8C7\"]\n",
    ")\n",
    "plt.title(\"Species Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba7cd5c",
   "metadata": {},
   "source": [
    "- There are more dog images (67.7%, 4978) than cat images (32.3%, 2371), which highlights an important class imbalance.\n",
    "\n",
    "- For the binary classification (cat vs dog) :  we will need to either use a weighted loss so the network penalizes mistakes on cats more fairly, oversample cats or undersample dogs, or maybe apply stronger augmentation to cats. We will also need to consider per-class metrics (precision per class, recall per class, f1-score, and not only the accuracy that may hide some bias)\n",
    "\n",
    "- For the multiclass classification (37 breeds) : We have already constructed the train/val/test sets with a stratified splitting so that each breed appears with the correct (80/10/10) proportion. We can also consider using a weighted loss, data augmentation, and evaluate our model with macro metrics (macro F1, macro recall, balanced accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8901f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_crosstab = pd.crosstab(df[\"species_name\"], df[\"new_split\"])\n",
    "propertion_per_specie = species_crosstab.div(species_crosstab.sum(axis=1), axis=0)\n",
    "species_crosstab\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(propertion_per_specie, annot=True, fmt=\".3f\", cmap=\"Blues\")\n",
    "plt.title(\"Specie × Split — Normalized Proportions\")\n",
    "plt.xlabel(\"Split\")\n",
    "plt.ylabel(\"Specie\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba57cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = df[df[\"species_name\"] == \"Cat\"]\n",
    "\n",
    "plt.figure(figsize=(14,12))\n",
    "ax = sns.countplot(\n",
    "    data=cats,\n",
    "    y=\"breed_name\",\n",
    "    order=cats[\"breed_name\"].value_counts().index,\n",
    "    color=\"#F5E8C7\"\n",
    ")\n",
    "plt.title(\"Distribution of Cat Breeds\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Breed\")\n",
    "\n",
    "#Bar Annotations\n",
    "total = len(cats)\n",
    "\n",
    "for p in ax.patches:\n",
    "    count = int(p.get_width())\n",
    "    percent = 100 * count / total\n",
    "    text = f\"{count} ({percent:.1f}%)\"\n",
    "    ax.text(\n",
    "        p.get_width() - 20,     \n",
    "        p.get_y() + p.get_height()/2, \n",
    "        text,\n",
    "        va=\"center\"\n",
    "    )\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d908fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(proportion_per_breed.iloc[:12], annot=True, fmt=\".3f\", cmap=\"Blues\")\n",
    "plt.title(\"Breed × Split — Normalized Proportions (Cats only)\")\n",
    "plt.xlabel(\"Split\")\n",
    "plt.ylabel(\"Breed\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133db380",
   "metadata": {},
   "outputs": [],
   "source": [
    "dogs = df[df[\"species_name\"] == \"Dog\"]\n",
    "\n",
    "plt.figure(figsize=(14,12))\n",
    "ax = sns.countplot(\n",
    "    data=dogs,\n",
    "    y=\"breed_name\",\n",
    "    order=dogs[\"breed_name\"].value_counts().index,\n",
    "    color=\"#A7D8F0\"\n",
    ")\n",
    "plt.title(\"Distribution of Dog Breeds\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Breed\")\n",
    "\n",
    "#Bar Annotations\n",
    "total = len(dogs)\n",
    "\n",
    "for p in ax.patches:\n",
    "    count = int(p.get_width())\n",
    "    percent = 100 * count / total\n",
    "    text = f\"{count} ({percent:.1f}%)\"\n",
    "    ax.text(\n",
    "        p.get_width() - 20,     \n",
    "        p.get_y() + p.get_height()/2, \n",
    "        text,\n",
    "        va=\"center\"\n",
    "    )\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a984a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(proportion_per_breed.iloc[12:], annot=True, fmt=\".3f\", cmap=\"Blues\")\n",
    "plt.title(\"Breed × Split — Normalized Proportions (Dogs only)\")\n",
    "plt.xlabel(\"Split\")\n",
    "plt.ylabel(\"Breed\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd5ff4f",
   "metadata": {},
   "source": [
    "There are 12 breeds of cats and 25 breeds of dogs.\n",
    "\n",
    "All races have 200 images each, except for :\n",
    "\n",
    "Cats : \n",
    " - Siameses with 199 images\n",
    " - Abyssinians with 198 images\n",
    " - Egyptian_Maus with 190 images\n",
    " - Bombays with 184 images\n",
    " \n",
    "Dogs : \n",
    " - boxers with 199 images\n",
    " - keeshonds with 199 images\n",
    " - scottish_terriers with 199 images\n",
    " - english_cocker_spaniels with 196 images\n",
    " - newfoundlands with 196 images\n",
    "- staffordshire_bull_terriers with 189 images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d0570",
   "metadata": {},
   "source": [
    "### Visualisation of Image, Masks, and Bounding Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee5e77c",
   "metadata": {},
   "source": [
    "#### Image and masks and mask visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae03f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_mask_bbox(class_id):\n",
    "    row = df.loc[class_id] if class_id in df.index else df[df[\"class_id\"] == class_id].iloc[0]\n",
    "\n",
    "    img = Image.open(row[\"img_path\"]).convert(\"RGB\")\n",
    "    ann = Image.open(row[\"trimap_path\"])\n",
    "\n",
    "    img_arr = np.array(img)\n",
    "    ann_arr = np.array(ann)\n",
    "\n",
    "    if img_arr.shape[:2] == ann_arr.shape[:2]:\n",
    "        print(\"The mask and image have the same size\")\n",
    "    else:\n",
    "        print(\"Warning: image and mask sizes differ:\",\n",
    "              img_arr.shape[:2], ann_arr.shape[:2])\n",
    "\n",
    "    #Contour \n",
    "    contour = img_arr.copy()\n",
    "    contour[ann_arr == 3] = [0, 255, 0] # we highlight the contour\n",
    "    has_xml = not pd.isna(row[\"xml_path\"])\n",
    "    n_panels = 4 if has_xml else 3\n",
    "\n",
    "    fig, ax = plt.subplots(1, n_panels, figsize=(5 * n_panels, 5))\n",
    "\n",
    "    # Original image\n",
    "    ax[0].imshow(img)\n",
    "    ax[0].set_title(row[\"class_id\"])\n",
    "    ax[0].axis(\"off\")\n",
    "\n",
    "    # Annotation from trimap\n",
    "    ax[1].imshow(ann_arr, cmap=\"viridis\")\n",
    "    ax[1].set_title(\"Annotation\")\n",
    "    ax[1].axis(\"off\")\n",
    "\n",
    "    # Contour\n",
    "    ax[2].imshow(contour)\n",
    "    ax[2].set_title(\"Contour\")\n",
    "    ax[2].axis(\"off\")\n",
    "\n",
    "    # Bounding box (if the .xml file exists)\n",
    "    if has_xml:\n",
    "        x_min = int(row[\"bb_xmin\"])\n",
    "        y_min = int(row[\"bb_ymin\"])\n",
    "        x_max = int(row[\"bb_xmax\"])\n",
    "        y_max = int(row[\"bb_ymax\"])\n",
    "        box_w = x_max - x_min\n",
    "        box_h = y_max - y_min\n",
    "\n",
    "        ax[3].imshow(img_arr)\n",
    "        rect = patches.Rectangle(\n",
    "            (x_min, y_min),\n",
    "            box_w,\n",
    "            box_h,\n",
    "            linewidth=4,\n",
    "            edgecolor=\"lime\",\n",
    "            facecolor=\"none\"\n",
    "        )\n",
    "        ax[3].add_patch(rect)\n",
    "        ax[3].set_title(\"Bounding box\")\n",
    "        ax[3].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Abyssinian_1 has an .xml file\n",
    "show_image_mask_bbox(\"Abyssinian_1\")\n",
    "\n",
    "# Abyssinian_1 has an .xml file\n",
    "show_image_mask_bbox(\"samoyed_100\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebebe44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_mask_bbox(\"Bengal_111\")\n",
    "show_image_mask_bbox(\"Bengal_175\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dfd648",
   "metadata": {},
   "source": [
    "- When a trimap is available, it provides information about both the animal and the background.\n",
    "- The contour may be fully visible in the image (as in Abyssinian_1) or only partially visible (as in samoyed_100).\n",
    "- Some images contain two animals, and in those cases the trimap may include either a single merged contour (e.g., Bengal_111) or two separate contours (e.g., Bengal_175).\n",
    "- We also observed that when a bounding box is provided, it typically encloses only the animal's head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bad1e4",
   "metadata": {},
   "source": [
    "#### Missing bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1367cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing bounding boxes\n",
    "missing_xml = df[df[\"xml_path\"].isna()]\n",
    "num_missing = len(missing_xml)\n",
    "print(f\"Number of images with missing XML/bounding boxes: {num_missing}\")\n",
    "\n",
    "sample_missing = missing_xml.sample(3, random_state=42)\n",
    "for class_id in sample_missing[\"class_id\"]:\n",
    "    print(f\"Showing: {class_id}\")\n",
    "    show_image_mask_bbox(class_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0977fb6e",
   "metadata": {},
   "source": [
    "##### Missing trimaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f12d92b",
   "metadata": {},
   "source": [
    "- We found that 14 images have trimaps containing only a single color.\n",
    "- This is not an issue for the classification tasks, but these images will be removed before segmentation to avoid introducing bias into the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62937615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing trimaps \n",
    "bad_trimaps = []\n",
    "for class_id, row in df.iterrows():\n",
    "    trimap_path = row[\"trimap_path\"]\n",
    "    ann = Image.open(trimap_path)\n",
    "    ann_arr = np.array(ann)\n",
    "    uniq = np.unique(ann_arr)\n",
    "\n",
    "    if len(uniq) != 3 or not set(uniq).issubset({1, 2, 3}):\n",
    "        bad_trimaps.append((class_id, uniq))\n",
    "\n",
    "print(\"Trimaps with unexpected number of colors:\")\n",
    "for cid, uniq in bad_trimaps:\n",
    "    print(f\"{cid}: unique values = {uniq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914f131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 images with unexpected trimaps\n",
    "show_image_mask_bbox(\"miniature_pinscher_14\")\n",
    "show_image_mask_bbox(\"japanese_chin_199\")\n",
    "show_image_mask_bbox(\"keeshond_7\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06067c21",
   "metadata": {},
   "source": [
    "#### Random batches of overlayed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12998891",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_nine = df.sample(9,random_state=-0)\n",
    "plt.figure(figsize=(14, 14))\n",
    "for i, (class_id, row) in enumerate(sample_nine.iterrows(), start=1):\n",
    "\n",
    "    img = Image.open(row[\"img_path\"]).convert(\"RGB\")\n",
    "    mask = Image.open(row[\"trimap_path\"])\n",
    "\n",
    "    img_arr = np.array(img)\n",
    "    mask_arr = np.array(mask)\n",
    "\n",
    "    overlay = img_arr.copy()\n",
    "    overlay[mask_arr == 3] = [0, 255, 0] \n",
    "\n",
    "    species = row[\"species_name\"].capitalize()    \n",
    "    breed = row[\"breed_name\"].replace(\"_\", \" \").title()   \n",
    "    title = f\"{species} — {class_id}\"\n",
    "\n",
    "    plt.subplot(3, 3, i)\n",
    "    plt.imshow(overlay)\n",
    "    plt.title(title, fontsize=10)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86426c31",
   "metadata": {},
   "source": [
    "- We note that some images are taken indoors (e.g., Abyssinian_136), some outdoors (e.g., leonberger_130), and some against a uniform background (e.g., Egyptian_Mau_39).\n",
    "- From studying the image masks, we have observed that the masking is done very well. No masks are missing and there is no mask and image mismatch. When drawing random images from the database, we see that the masking includes details such as collars, toys and objects that interfere with the animal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed11e18",
   "metadata": {},
   "source": [
    "### Width and Height analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a45d3",
   "metadata": {},
   "source": [
    "- The .xml files provide the width and height, but as they are missing more nearly half of the images, we need to calculate again the width and height for each image, and update the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe29557",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"width\", \"height\"])\n",
    "\n",
    "widths = []\n",
    "heights = []\n",
    "\n",
    "for path in df[\"img_path\"]:\n",
    "    img = Image.open(path)\n",
    "    w, h = img.size\n",
    "    widths.append(w)\n",
    "    heights.append(h)\n",
    "\n",
    "df[\"width\"] = widths\n",
    "df[\"height\"] = heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee718cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"width\"], bins=20, label= \"Widths\", alpha= 0.6)\n",
    "plt.hist(df[\"height\"], bins=20, color = \"red\", label = \"Heights\", alpha= 0.6)\n",
    "plt.title(\"Image Size Distribution\")\n",
    "plt.xlabel(\"Pixel height/width\")\n",
    "plt.ylabel(\"Nb of images\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00019c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area (width × height)\n",
    "area_df = df.dropna(subset=[\"width\", \"height\"]).copy()\n",
    "\n",
    "area_df[\"width\"] = area_df[\"width\"].astype(int)\n",
    "area_df[\"height\"] = area_df[\"height\"].astype(int)\n",
    "\n",
    "area_df[\"area\"] = area_df[\"width\"] * area_df[\"height\"]\n",
    "min_area  = area_df[\"area\"].min()\n",
    "max_area  = area_df[\"area\"].max()\n",
    "mean_area = area_df[\"area\"].mean()\n",
    "\n",
    "print(f\"Smallest area : {min_area} pixels\")\n",
    "print(f\"Largest area  : {max_area} pixels\")\n",
    "print(f\"Mean area     : {mean_area:.2f} pixels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4005dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x=\"width\",\n",
    "    y=\"height\",\n",
    "    alpha=0.3,\n",
    "    s=30,\n",
    "    edgecolor=None\n",
    ")\n",
    "plt.title(\"Width vs Height\")\n",
    "plt.xlabel(\"Width (pixels)\")\n",
    "plt.ylabel(\"Height (pixels)\")\n",
    "plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb881e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"width\", \"height\"]).size().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d090d",
   "metadata": {},
   "source": [
    "- We observe that the size of the images vary significantly. We will have to normalise the image size before training our models in the next sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd29cb",
   "metadata": {},
   "source": [
    "We will now study bias of the images. Perhaps some breeds are more often photographed outside rather than inside. There could possibly be more background compared to foreground from some breeds as well. The lightning of the images could also differ from breed to breed or between dogs and cats. For this reason, we will study: \n",
    "\n",
    "- The average brightness of the images per species and breed. This is quantified by the mean pixel intensity. Source: https://www.thecolorcode.net/post/20/\n",
    "- The average green intensity per species and breed\n",
    "- The foreground ratio per species and breed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7865e626",
   "metadata": {},
   "source": [
    "### Brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dff32f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"avg_brightness\"] = df[\"class_id\"].apply(\n",
    "    lambda name: np.array(Image.open(f\"{PATH_TO_IMG}{name}.jpg\").convert(\"L\")).mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735497a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_label_specie = (\n",
    "    df.groupby(\"species_name\")[\"avg_brightness\"]\n",
    "      .median()\n",
    "      .sort_values(ascending=False)\n",
    "      .index\n",
    ")\n",
    "\n",
    "df[\"species_name\"] = pd.Categorical(df[\"species_name\"],categories=order_label_specie,ordered=True)\n",
    "\n",
    "df.boxplot(column=\"avg_brightness\",by=\"species_name\",patch_artist=True)\n",
    "\n",
    "plt.title(\"Brightness by Species\")\n",
    "plt.xlabel(\"Species\")\n",
    "plt.ylabel(\"Average brightness\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e3b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_label_breed = (\n",
    "    df.groupby(\"breed_name\")[\"avg_brightness\"]\n",
    "      .median()\n",
    "      .sort_values(ascending=False)\n",
    "      .index\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "sns.boxplot(\n",
    "    data=df,\n",
    "    x=\"breed_name\",\n",
    "    y=\"avg_brightness\",\n",
    "    order=order_label_breed,\n",
    "    showcaps=True,\n",
    "    boxprops={\"facecolor\":\"lightblue\"},\n",
    "    medianprops={\"color\":\"red\", \"linewidth\":2},\n",
    ")\n",
    "\n",
    "plt.title(\"Brightness by Breeds\")\n",
    "plt.xlabel(\"Breeds\")\n",
    "plt.ylabel(\"Average brightness\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0539130c",
   "metadata": {},
   "source": [
    "When observing the average brightness of each species, we see that the average brightness is the highest for staffordshire bull terrier dogs and the lowest for Bombay cats. There is also a sightly higher brightness for images of dogs. This could be because some images of breeds are taken outside or in the sun rather than inside or during the night. \n",
    "This could affect our results in several ways: \n",
    "- when some breeds are systematically brighter than others, it could be used as a shortcut feature rather than the actual caracteristict of the species/breed. \n",
    "- Very bright or very dark images can be harder to perform segmentation on, as it's harder to recognise contrasts and boundaries on them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e15a31e",
   "metadata": {},
   "source": [
    "### Green level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687133a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_green_level(image_name):\n",
    "    img_path = f\"{PATH_TO_IMG}/{image_name}.jpg\"\n",
    "    img = np.array(Image.open(img_path))\n",
    "    return img[:, :, 1].mean()     # channel 1 = green\n",
    "\n",
    "df[\"avg_green\"] = df[\"class_id\"].apply(average_green_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9901ca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"breed_name\")[\"avg_green\"].mean().nlargest(37).plot(kind=\"bar\")\n",
    "plt.title(\"Green intensity of images in breeds\")\n",
    "plt.ylabel(\"Green Intensity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be44bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"species_name\")[\"avg_green\"].mean().plot(kind=\"bar\")\n",
    "plt.title(\"Green intensity of images in species\")\n",
    "plt.ylabel(\"Green Intensity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b37d2",
   "metadata": {},
   "source": [
    "We observe that the images of dogs have a higher green intensity than that of cats. Dogs seem to be more likely to be captured next to grass or trees. In the same way as with the brightness, we see that the green intensity is the highest for staffordshire bull terrier dogs and the lowest for Bombay cats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c1d8d8",
   "metadata": {},
   "source": [
    "### Foreground ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d9a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "foreground_ratios = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    mask_file = f\"{row['class_id']}.png\"\n",
    "    mask = np.array(Image.open(f\"{PATH_TO_TRIMAPS}/{mask_file}\"))\n",
    "    fg_ratio = (mask == 1).mean()\n",
    "    foreground_ratios.append(fg_ratio)\n",
    "\n",
    "df[\"fg_ratio\"] = foreground_ratios\n",
    "\n",
    "df.boxplot(column=\"fg_ratio\", by=\"species_name\")\n",
    "plt.title(\"Foreground Ratio by Species\")\n",
    "plt.suptitle(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef60c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_label_breed_fg = (\n",
    "    df.groupby(\"breed_name\")[\"fg_ratio\"].median().sort_values(ascending=False).index\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(\n",
    "    data=df,\n",
    "    x=\"breed_name\",\n",
    "    y=\"fg_ratio\",\n",
    "    order=order_label_breed_fg\n",
    ")\n",
    "\n",
    "plt.title(\"Foreground ratio by breed\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09cc669",
   "metadata": {},
   "source": [
    "We observe that the images of cats have a higher proportion of the actual animal in the images. This could lead to better learning of features for cats, as less of the background can be used as shortcut features. The model can focus on object features. However, it could also be a feature shortcut in itself that images of cats are more \"zoomed in\", then it could be harder to classify pictures of cats taken from far apart. The fact that be observe clear differences in breeds and species could make our models implicitly learn size-based bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da23e3",
   "metadata": {},
   "source": [
    "### Conclusion on the Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044dfadf",
   "metadata": {},
   "source": [
    "From the exploratory analysis of the Oxford-IIIT pet dataset, we have observed that there are several imbalances to take into account. Firstly, there are more images, as well as breeds, of dogs than cats. The proportion of images per breed is almost equally distributed. Secondly, the masks of the dataset are well placed. The bounding boxed typically contain the heads of the animals. Thirdly, we have observed bias in the dataset when it comes to the brightness, the amount of green and how much of the images contain the animal vs. background. Lastly, we have seen that the widt and heights of the images are not equal.  \n",
    "Because of this, we will normalise the images by size and colour. We will keep the observed biases in mind when analysing the final results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217287bb",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735c2660",
   "metadata": {},
   "source": [
    "### Dataset construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6716bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resizing images to 224*224 pixels. This seems to be common for similar projects. \n",
    "IMG_SIZE = 224\n",
    "# We use the mean colours from image net to normalise the colours of the images. \n",
    "IMAGE_NET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGE_NET_STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1f29e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df[\"new_split\"] == \"train\"].reset_index(drop=True)\n",
    "val_df   = df[df[\"new_split\"] == \"val\"].reset_index(drop=True)\n",
    "test_df  = df[df[\"new_split\"] == \"test\"].reset_index(drop=True)\n",
    "print(len(train_df), len(val_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524497fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7054f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = set(train_df[\"breed_id\"].unique())\n",
    "val_classes   = set(val_df[\"breed_id\"].unique())\n",
    "\n",
    "print(\"Nombre de classes dans train :\", len(train_classes))\n",
    "print(\"Nombre de classes dans val   :\", len(val_classes))\n",
    "\n",
    "missing_in_val = sorted(train_classes - val_classes)\n",
    "missing_in_train = sorted(val_classes - train_classes)\n",
    "\n",
    "print(\"Classes ABSENTES du validation :\", missing_in_val)\n",
    "print(\"Classes ABSENTES de l'entraînement :\", missing_in_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6a49e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetDataset(Dataset):\n",
    "    def __init__(self, df, task=\"binary\", transform=None):\n",
    "        \"\"\"\n",
    "        task = \"binary\" (0=cat, 1=dog) (binary classification)\n",
    "        task = \"multiclass\"   (0..36) (multiclass classification)\n",
    "        task = \"segmentation\" (returns mask + image)\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.task = task\n",
    "        self.transform = transform\n",
    "        \n",
    "        # To remove the 14 images without trimaps when doing segmentation\n",
    "        if task == \"segmentation\":\n",
    "            self.df = self.df[self.df[\"trimap_path\"].notna()].reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        image = Image.open(row[\"img_path\"]).convert(\"RGB\")\n",
    "        filename = os.path.basename(row[\"img_path\"])\n",
    "\n",
    "        if self.task == \"binary\":\n",
    "            label = row[\"species\"]    # 0=cat, 1=dog\n",
    "\n",
    "        elif self.task == \"multiclass\":\n",
    "            label = row[\"id\"]   # 0..36\n",
    "\n",
    "        elif self.task == \"segmentation\":\n",
    "            mask = Image.open(row[\"trimap_path\"])\n",
    "            if self.transform:\n",
    "                transformed = self.transform(image=np.array(image), mask=np.array(mask))\n",
    "                return transformed[\"image\"], transformed[\"mask\"]\n",
    "            return np.array(image), np.array(mask)\n",
    "\n",
    "        # Transformation (for classification only)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, filename\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc13bbb",
   "metadata": {},
   "source": [
    "### Default classification tranform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34076bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = T.Compose([\n",
    "    T.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean = IMAGE_NET_MEAN,\n",
    "                std = IMAGE_NET_STD),\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean = IMAGE_NET_MEAN,\n",
    "                std = IMAGE_NET_STD),\n",
    "])\n",
    "\n",
    "train_transform_aug = T.Compose([\n",
    "    T.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2,\n",
    "                  saturation=0.2, hue=0.02),\n",
    "    T.RandomRotation(degrees=10),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=IMAGE_NET_MEAN, std=IMAGE_NET_STD),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bab54a",
   "metadata": {},
   "source": [
    "### Binary classification instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_binary_clf = PetDataset(train_df, task=\"binary\", transform=train_transform) #without augmentation\n",
    "train_binary_clf_aug = PetDataset(train_df, task=\"binary\", transform=train_transform_aug) #with augmentation\n",
    "\n",
    "val_binary_clf  = PetDataset(val_df, task=\"binary\", transform=val_transform)\n",
    "test_binary_clf  = PetDataset(test_df, task=\"binary\", transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00619657",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_binary_clf = DataLoader(train_binary_clf, batch_size=32, shuffle=True,  num_workers=0) #without aug\n",
    "train_loader_binary_clf_aug =  DataLoader(train_binary_clf_aug, batch_size=32, shuffle=True,  num_workers=0) #with aug\n",
    "\n",
    "val_loader_binary_clf   = DataLoader(val_binary_clf,   batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaa67b4",
   "metadata": {},
   "source": [
    "### Multiclass classification instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7766ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_multi_clf = PetDataset(train_df, task=\"multiclass\", transform=train_transform) #without augmentation\n",
    "train_multi_clf_aug = PetDataset(train_df, task=\"multiclass\", transform=train_transform_aug) #with augmentation\n",
    "\n",
    "val_multi_clf   = PetDataset(val_df, task=\"multiclass\", transform=val_transform)\n",
    "test_multi_clf  = PetDataset(test_df, task=\"multiclass\", transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e02e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_multi_clf= DataLoader(train_multi_clf, batch_size=32, shuffle=True,  num_workers=0)\n",
    "train_loader_multi_clf_aug = DataLoader(train_multi_clf_aug, batch_size=32, shuffle=True,  num_workers=0)\n",
    "\n",
    "val_loader_multi_clf   = DataLoader(val_multi_clf,   batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a77ca",
   "metadata": {},
   "source": [
    "### Segmentation instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be determined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d69639b",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f718083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val_loss(history):\n",
    "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(epochs, history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy(history):\n",
    "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(epochs, history[\"train_acc\"], label=\"Train Accuracy\")\n",
    "    plt.plot(epochs, history[\"val_acc\"], label=\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def classification_metrics(y_true, y_prob, class_names=None, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Works for both binary and multiclass classification.\n",
    "\n",
    "    Assumptions:\n",
    "    - y_true are integer class labels in {0, 1, ..., n_classes-1}\n",
    "    - y_prob can be:\n",
    "        * shape (N,)  -> probability of the positive class in binary case\n",
    "        * shape (N,1) -> same as above\n",
    "        * shape (N,C) -> class probabilities for C classes\n",
    "    \"\"\"\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = np.array(y_prob)\n",
    "\n",
    "    # ---------- Standardize y_prob to 2D ----------\n",
    "    if y_prob.ndim == 1:\n",
    "        # Assume binary, y_prob is P(class 1)\n",
    "        y_prob_pos = y_prob\n",
    "        y_prob = np.column_stack([1 - y_prob_pos, y_prob_pos])  # (N, 2)\n",
    "    elif y_prob.ndim == 2 and y_prob.shape[1] == 1:\n",
    "        # (N,1) -> (N,2)\n",
    "        y_prob_pos = y_prob[:, 0]\n",
    "        y_prob = np.column_stack([1 - y_prob_pos, y_prob_pos])  # (N, 2)\n",
    "    # else: assume already (N, n_classes)\n",
    "\n",
    "    n_samples, n_classes = y_prob.shape\n",
    "\n",
    "    # ---------- Class names ----------\n",
    "    if class_names is None:\n",
    "        # assume labels are 0..n_classes-1\n",
    "        class_names = [str(i) for i in range(n_classes)]\n",
    "\n",
    "    # ---------- Predictions & ROC AUC ----------\n",
    "    if n_classes == 2:\n",
    "        # Binary: use probability of class 1 (positive class)\n",
    "        y_scores_pos = y_prob[:, 1]\n",
    "        y_pred = (y_scores_pos >= threshold).astype(int)\n",
    "        roc_auc = roc_auc_score(y_true, y_scores_pos)\n",
    "    else:\n",
    "        # Multiclass: argmax\n",
    "        y_pred = np.argmax(y_prob, axis=1)\n",
    "        roc_auc = roc_auc_score(y_true, y_prob, multi_class=\"ovr\")\n",
    "\n",
    "    # ---------- Confusion matrix & classification report ----------\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        target_names=class_names,\n",
    "        digits=3,\n",
    "    )\n",
    "\n",
    "    # ---------- Per-class PR curves & AP ----------\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    pr_thresholds = {}\n",
    "    ap = {}\n",
    "\n",
    "    # y_true must be in 0..n_classes-1 for this to work\n",
    "    y_true_bin = np.eye(n_classes)[y_true]  # shape (N, n_classes)\n",
    "\n",
    "    for i, cname in enumerate(class_names):\n",
    "        precision[cname], recall[cname], pr_thresholds[cname] = precision_recall_curve(\n",
    "            y_true_bin[:, i], y_prob[:, i]\n",
    "        )\n",
    "        ap[cname] = average_precision_score(y_true_bin[:, i], y_prob[:, i])\n",
    "\n",
    "    return {\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"classification_report\": report,\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"precision\": precision,           # dict: class_name -> array\n",
    "        \"recall\": recall,                 # dict: class_name -> array\n",
    "        \"pr_thresholds\": pr_thresholds,   # dict: class_name -> array\n",
    "        \"average_precision\": ap,          # dict: class_name -> scalar\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def classification_metrics_multiclass(y_true, y_prob, class_names=None):\n",
    "    \"\"\"\n",
    "    Metrics for multiclass classification.\n",
    "\n",
    "    Assumptions:\n",
    "    - y_true are integer class labels in {0, 1, ..., n_classes-1}\n",
    "    - y_prob has shape (N, C) with C = number of model classes,\n",
    "      and columns correspond to class indices 0..C-1 in that order.\n",
    "    \"\"\"\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = np.array(y_prob)\n",
    "\n",
    "    if y_prob.ndim != 2:\n",
    "        raise ValueError(\n",
    "            f\"Expected y_prob to have shape (N, C) for multiclass, got shape {y_prob.shape}\"\n",
    "        )\n",
    "\n",
    "    n_samples, n_classes = y_prob.shape\n",
    "\n",
    "    # ---------- Class names ----------\n",
    "    if class_names is None:\n",
    "        class_names = [str(i) for i in range(n_classes)]\n",
    "    else:\n",
    "        assert len(class_names) == n_classes, \\\n",
    "            f\"len(class_names)={len(class_names)} does not match n_classes={n_classes}\"\n",
    "\n",
    "    # ---------- Predictions ----------\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "    # ---------- Confusion matrix & classification report ----------\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    report = classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        labels=np.arange(n_classes),   # force all classes 0..C-1\n",
    "        target_names=class_names,\n",
    "        digits=3,\n",
    "        zero_division=0,\n",
    "    )\n",
    "\n",
    "    # ---------- Multiclass ROC AUC (macro, One-vs-Rest) ----------\n",
    "    try:\n",
    "        roc_auc_macro_ovr = roc_auc_score(\n",
    "            y_true,\n",
    "            y_prob,\n",
    "            multi_class=\"ovr\",\n",
    "            average=\"macro\",\n",
    "            labels=np.arange(n_classes),\n",
    "        )\n",
    "    except ValueError:\n",
    "        roc_auc_macro_ovr = None\n",
    "\n",
    "    # ---------- Per-class PR curves & AP ----------\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    pr_thresholds = {}\n",
    "    ap_per_class = {}\n",
    "\n",
    "    # y_true one-hot\n",
    "    y_true_bin = np.eye(n_classes)[y_true]  # shape (N, n_classes)\n",
    "\n",
    "    for i, cname in enumerate(class_names):\n",
    "        precision[cname], recall[cname], pr_thresholds[cname] = precision_recall_curve(\n",
    "            y_true_bin[:, i], y_prob[:, i]\n",
    "        )\n",
    "        ap_per_class[cname] = average_precision_score(\n",
    "            y_true_bin[:, i], y_prob[:, i]\n",
    "        )\n",
    "\n",
    "    # Macro-average AP\n",
    "    average_precision_macro = float(np.mean(list(ap_per_class.values())))\n",
    "\n",
    "    # For compatibility with save_model_and_results:\n",
    "    # - \"roc_auc\" will be the macro OVR ROC AUC\n",
    "    # - \"average_precision\" will be the per-class AP dict\n",
    "    return {\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"classification_report\": report,\n",
    "        \"roc_auc_macro_ovr\": roc_auc_macro_ovr,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"pr_thresholds\": pr_thresholds,\n",
    "        \"average_precision_per_class\": ap_per_class,\n",
    "        \"average_precision_macro\": average_precision_macro,\n",
    "        # keys expected by save_model_and_results\n",
    "        \"roc_auc\": roc_auc_macro_ovr,\n",
    "        \"average_precision\": ap_per_class,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, title=\"Confusion Matrix\"):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(y_true, y_prob, title=\"ROC Curve\"):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(fpr, tpr, label=\"ROC\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_pr_curve(y_true, y_prob, title=\"Precision–Recall Curve\"):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    ap = average_precision_score(y_true, y_prob)\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(recall, precision, label=f\"PR (AP={ap:.3f})\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3abddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    for imgs, labels, _ in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels, _ in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "            probs = F.softmax(outputs, dim=1)[:, 1]  # probability of class \"1\" (dog)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    acc = correct / total\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "\n",
    "    return avg_loss, acc, all_labels, all_probs\n",
    "\n",
    "def evaluate_multiclass(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels, _ in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "            # MULTICLASS: keep ALL class probabilities\n",
    "            probs = F.softmax(outputs, dim=1)   # shape [B, num_classes]\n",
    "            preds = outputs.argmax(dim=1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    acc = correct / total\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)     # shape [N]\n",
    "    all_probs = np.concatenate(all_probs)       # shape [N, num_classes]\n",
    "\n",
    "    return avg_loss, acc, all_labels, all_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca10674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_image(tensor):\n",
    "    \"\"\"Undo normalization so we can display the image.\"\"\"\n",
    "    img = tensor.cpu().numpy().transpose(1,2,0)\n",
    "    img = img * IMAGE_NET_STD + IMAGE_NET_MEAN\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "def show_random_classif_predictions(model, dataset, device, class_names, num_images=9):\n",
    "    model.eval()\n",
    "\n",
    "    indices = random.sample(range(len(dataset)), num_images)\n",
    "\n",
    "    # Adapt grid size for visualization (e.g., square root)\n",
    "    grid_size = int(num_images ** 0.5)\n",
    "    plt.figure(figsize=(grid_size * 4, grid_size * 4))\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        img, label, filename = dataset[idx]\n",
    "        img_tensor = img.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(img_tensor)\n",
    "            probs = F.softmax(logits, dim=1)[0]  # shape: [num_classes]\n",
    "            pred = torch.argmax(probs).item()\n",
    "\n",
    "        img_vis = denormalize_image(img)\n",
    "\n",
    "        plt.subplot(grid_size, grid_size, i + 1)\n",
    "        plt.imshow(img_vis)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Compose prediction probabilities string for all classes\n",
    "        prob_str = \", \".join([f\"P_{name}={probs[j]:.2f}\" for j, name in enumerate(class_names)])\n",
    "\n",
    "        pred_name = class_names[pred]\n",
    "        true_name = class_names[label]\n",
    "\n",
    "        title = (\n",
    "            f\"{filename}\\n\"\n",
    "            f\"Pred: {pred_name}\\n\"\n",
    "            f\"{prob_str}\\n\"\n",
    "            f\"True: {true_name}\"\n",
    "        )\n",
    "\n",
    "        plt.title(title, fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_random_classif_predictions_topk(\n",
    "    model,\n",
    "    dataset,\n",
    "    device,\n",
    "    class_names,\n",
    "    num_images=9,\n",
    "    k=3,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    indices = random.sample(range(len(dataset)), num_images)\n",
    "\n",
    "    # compute grid size more robustly\n",
    "    grid_size = int(np.ceil(num_images ** 0.5))\n",
    "    plt.figure(figsize=(grid_size * 4, grid_size * 4))\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        img, label, filename = dataset[idx]\n",
    "        img_tensor = img.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(img_tensor)\n",
    "            probs = F.softmax(logits, dim=1)[0]  # shape: [num_classes]\n",
    "            pred = torch.argmax(probs).item()\n",
    "\n",
    "        img_vis = denormalize_image(img)\n",
    "\n",
    "        plt.subplot(grid_size, grid_size, i + 1)\n",
    "        plt.imshow(img_vis)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # top-k predictions\n",
    "        topk_vals, topk_idx = torch.topk(probs, k)\n",
    "        topk_vals = topk_vals.cpu().numpy()\n",
    "        topk_idx = topk_idx.cpu().numpy()\n",
    "\n",
    "        topk_str_lines = []\n",
    "        for rank, (cidx, p) in enumerate(zip(topk_idx, topk_vals), start=1):\n",
    "            cname = class_names[cidx]\n",
    "            topk_str_lines.append(f\"{rank}) {cname}: {p:.2f}\")\n",
    "\n",
    "        pred_name = class_names[pred]\n",
    "        true_name = class_names[label]\n",
    "\n",
    "        title = (\n",
    "            f\"{filename}\\n\"\n",
    "            f\"Pred: {pred_name} | True: {true_name}\\n\"\n",
    "            + \"\\n\".join(topk_str_lines)\n",
    "        )\n",
    "\n",
    "        plt.title(title, fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c03aa",
   "metadata": {},
   "source": [
    "### Function to load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb813dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2007a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_results(\n",
    "    save_dir: str,\n",
    "    model: torch.nn.Module,\n",
    "    history: dict,\n",
    "    y_true: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    metrics: dict,\n",
    "    model_filename: str = \"model_weights.pth\",\n",
    "):\n",
    "    # Create the full directory path you passed\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # --- Save model weights ---\n",
    "    model_path = os.path.join(save_dir, model_filename)\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    # --- Save training history ---\n",
    "    pd.DataFrame(history).to_csv(\n",
    "        os.path.join(save_dir, \"training_history.csv\"),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    # --- Save prediction arrays ---\n",
    "    np.save(os.path.join(save_dir, \"y_true.npy\"), y_true)\n",
    "    np.save(os.path.join(save_dir, \"y_prob.npy\"), y_prob)\n",
    "\n",
    "    # --- Save confusion matrix ---\n",
    "    np.save(\n",
    "        os.path.join(save_dir, \"confusion_matrix.npy\"),\n",
    "        metrics[\"confusion_matrix\"]\n",
    "    )\n",
    "\n",
    "    # --- Save metrics.json ---\n",
    "    avg_prec = metrics[\"average_precision\"]\n",
    "    if isinstance(avg_prec, dict):\n",
    "        # multiclass: already per-class, JSON-serializable\n",
    "        avg_prec_json = avg_prec\n",
    "    else:\n",
    "        # binary: single float\n",
    "        avg_prec_json = float(avg_prec)\n",
    "\n",
    "    metrics_for_json = {\n",
    "        \"roc_auc\": float(metrics[\"roc_auc\"]),\n",
    "        \"classification_report\": metrics[\"classification_report\"],\n",
    "        \"average_precision\": avg_prec_json,\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(save_dir, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump(metrics_for_json, f, indent=4)\n",
    "\n",
    "    print(f\"\\nArtifacts saved to: {save_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bd13d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_results(\n",
    "    save_dir: str,\n",
    "    model: torch.nn.Module,\n",
    "    device: str,\n",
    "    model_filename: str = \"model_weights.pth\",\n",
    "):\n",
    "    # --- Load model ---\n",
    "    model_path = os.path.join(save_dir, model_filename)\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    # --- Load history ---\n",
    "    history_df = pd.read_csv(os.path.join(save_dir, \"training_history.csv\"))\n",
    "\n",
    "    # --- Load predictions ---\n",
    "    y_true = np.load(os.path.join(save_dir, \"y_true.npy\"))\n",
    "    y_prob = np.load(os.path.join(save_dir, \"y_prob.npy\"))\n",
    "\n",
    "    # --- Load confusion matrix ---\n",
    "    cm = np.load(os.path.join(save_dir, \"confusion_matrix.npy\"))\n",
    "\n",
    "    # --- Load metrics.json ---\n",
    "    with open(os.path.join(save_dir, \"metrics.json\"), \"r\") as f:\n",
    "        metrics_json = json.load(f)\n",
    "\n",
    "    return model, history_df, y_true, y_prob, cm, metrics_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a07955b",
   "metadata": {},
   "source": [
    "## 2 - Binary (Species) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9df47f",
   "metadata": {},
   "source": [
    "### Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eef649",
   "metadata": {},
   "source": [
    "- [En cours] Simple CNN sans augmentation, sans ré-équilibrage des classes dans la loss\n",
    "- [To do] Simple CNN sans augmentation, avec ré-équilibrage des classes dans la loss\n",
    "- [To do] Simple CNN avec augmentation (avec ré-équilibrage directement ?) (--> à voir en fonction des résultats précedents)\n",
    "- [To do] Modèle pré-entraîné sans augmentation (avec ré-équilibrage directement ?) (--> à voir en fonction des résultats précedents)\n",
    "- [To do] Modèle pré-entraîné avec augmentation (avec ré-équilibrage directement ?) (--> à voir en fonction des résultats précedents)\n",
    "\n",
    "- Justifier choix optimizer, loss, hyperparameteres\n",
    "- Présenter les metrique à utiliser\n",
    "- Confusion matrix\n",
    "- Visualiser les loss (fixer 30 epochs ?)\n",
    "- Visualiser les bonnes prédictions, les erreurs\n",
    "- (si le temps) : Grad-CAM sur quelques images ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e505aca",
   "metadata": {},
   "source": [
    "### 2.1 - Binary Classification : CNN, Without augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_target_names = [\"Cat\",\"Dog\"]\n",
    "binary_class_names = (\"Cat\", \"Dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6295bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_V1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.fc1 = nn.Linear(512, 512)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1)) #global average pooling\n",
    "        self.fc2 = nn.Linear(512, 2)  # binary classif\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # 224 -> 112\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # 112 -> 56\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # 56 -> 28\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))  # 28 --> 14\n",
    "\n",
    "        x = self.gap(x) # [B, 512, 1, 1]                       \n",
    "        x = x.view(x.size(0), -1) # [B, 512]            \n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a65b68",
   "metadata": {},
   "source": [
    "#### 2.1.1 - Binary Classification : CNN, Without augmentation, No Class Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12126fb9",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41166dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\") # mps is for macs with Apple Silicon\n",
    "\n",
    "model_cnn_noaug_noclassweight = CNN_V1().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_cnn_noaug_noclassweight.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) #every 10 epoch, the lr is divided by 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f759817",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn_noaug_noclassweight = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "}\n",
    "\n",
    "for epoch in range(3):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model_cnn_noaug_noclassweight,\n",
    "        train_loader_binary_clf,\n",
    "        optimizer, criterion, device\n",
    "    )\n",
    "    \n",
    "    val_loss, val_acc, y_true, y_prob = evaluate(\n",
    "        model_cnn_noaug_noclassweight,\n",
    "        val_loader_binary_clf,\n",
    "        criterion, device\n",
    "    )\n",
    "\n",
    "    #scheduler.step()\n",
    "\n",
    "    history_cnn_noaug_noclassweight[\"train_loss\"].append(train_loss)\n",
    "    history_cnn_noaug_noclassweight[\"train_acc\"].append(train_acc)\n",
    "    history_cnn_noaug_noclassweight[\"val_loss\"].append(val_loss)\n",
    "    history_cnn_noaug_noclassweight[\"val_acc\"].append(val_acc)\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.3f} | \"\n",
    "          f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.3f}\")\n",
    "\n",
    "# Final eval, after training\n",
    "val_loss, val_acc, last_y_true, last_y_prob = evaluate(\n",
    "        model_cnn_noaug_noclassweight,\n",
    "        val_loader_binary_clf,\n",
    "        criterion, device\n",
    "    )\n",
    "\n",
    "y_true, y_prob = last_y_true, last_y_prob\n",
    "metrics_cnn_noaug_noclassweight = classification_metrics(y_true,y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1be16d",
   "metadata": {},
   "source": [
    "##### Save model weights, training info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_and_results(\n",
    "    save_dir=\"models/cnn_binary_noaug_nocw_noscheduler_test3epoch\",\n",
    "    model=model_cnn_noaug_noclassweight,\n",
    "    history=history_cnn_noaug_noclassweight,\n",
    "    y_true=y_true,\n",
    "    y_prob=y_prob,\n",
    "    metrics=metrics_cnn_noaug_noclassweight,\n",
    "    model_filename=\"cnn_binary_noaug_nocw_noscheduler_test3epoch.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad4ffa",
   "metadata": {},
   "source": [
    "##### Results (Losses & Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3708402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_noaug_noclassweight, history_cnn_noaug_noclassweight_df, y_true, y_prob, cm, metrics_cnn_noaug_noclassweight_json = load_model_and_results(\n",
    "    save_dir=\"models/cnn_binary_noaug_nocw_noscheduler_test3epoch\",\n",
    "    model=model_cnn_noaug_noclassweight,\n",
    "    device=device,\n",
    "    model_filename=\"cnn_binary_noaug_nocw_noscheduler_test3epoch.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b8ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n -- Classification report --\")\n",
    "print(metrics_cnn_noaug_noclassweight_json[\"classification_report\"])\n",
    "print(\"ROC AUC:\", metrics_cnn_noaug_noclassweight_json[\"roc_auc\"])\n",
    "print(\"AP (PR AUC):\", metrics_cnn_noaug_noclassweight_json[\"average_precision\"])\n",
    "\n",
    "plot_train_val_loss(history_cnn_noaug_noclassweight_df)\n",
    "plot_accuracy(history_cnn_noaug_noclassweight_df)\n",
    "plot_confusion_matrix(cm, (\"Cat\",\"Dog\"))\n",
    "plot_roc_curve(y_true, y_prob, title=\"ROC Curve – CNN no aug, no class weights\")\n",
    "plot_pr_curve(y_true, y_prob, title=\"PR Curve – CNN no aug, no class weights\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5808c431",
   "metadata": {},
   "source": [
    "##### Results (Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c615dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the validation set\n",
    "show_random_classif_predictions(model_cnn_noaug_noclassweight, val_binary_clf, device, class_names=binary_target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db9a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "show_random_classif_predictions(model_cnn_noaug_noclassweight, test_binary_clf, device, class_names=binary_target_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5397534f",
   "metadata": {},
   "source": [
    "| Task | Criterion | Optimizer | LR Scheduler | Model | Augmentation | Epochs | Train Loss | Val Loss | Train Acc | Val Acc | ROC AUC | PR AUC (AP) | Precision Cat | Recall Cat | Precision Dog | Recall Dog | F1-Score Cat | F1-Score Dog |Macro Precision | Macro Recall | Macro F1 |\n",
    "|------|-----------|-----------|--------|-----------|-----------|--------------|--------|------------|----------|-----------|----------|----------|--------------|----------------|-------------|----------------|-------------|---------------|----------------| ----------------| ----------------|\n",
    "| Binary Clf | CrossEntropyLoss | Adam (lr=1e-4, wd=1e-4) | No | CNN_V1 | None | 30 | 0.1661 | 0.3793 | 0.933 | 0.848 | 0.924 | 0.963 | 0.709 | 0.843 | 0.926 | 0.850 | 0.770 | 0.886 | 0.817 | 0.846 | 0.828 |\n",
    "| Binary Clf | CrossEntropyLoss | Adam (lr=1e-4, wd=1e-4) | step_size=10, gamma = 0.1 | CNN_V1 | None | 30 | 0.3440 | 0.4041 | 0.856 | 0.822 | 0.871 | 0.939 | 0.728 | 0.659 | 0.857 | 0.893 | 0.692 | 0.875 | NA | NA | NA |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98332e20",
   "metadata": {},
   "source": [
    "Dogs misclassified as Cats (non exhaustive list) :\n",
    "- shiba_inu_53 (test set)\n",
    "- boxer_61 (test set)\n",
    "\n",
    "Cats misclassified as Dogs (non exhaustive list) : \n",
    "- Russian_blue_118 (validation set)\n",
    "\n",
    "Comments on the metrics :\n",
    "* Val. accuracy : 84.8%, strong baseline\n",
    "* Macro F1 : 0.828 -> balanced performance across both classes\n",
    "* ROC AUC : 0.924 -> the model distinguishes well both classes\n",
    "* PR AUC (AP) : 0.964 -> the model makes highly confident predictions for the positive class\n",
    "* Cat/Dog Precision : 0.709 vs 0.926 -> the model makes more mistakes when predicting \"Cat\", very few false positives when predicting \"Dog\"\n",
    "* Cat/Dog Recall : 0.843 vs 0.850 -> the model catches most cats/dogs but with moderate false positives\n",
    "\n",
    "* Cats are the minority class, the model is biased toward predicting Dog. As Dogs dominate the training set, the model fits this class easily.\n",
    "* High precision shows better discrimination feature for dogs\n",
    "\n",
    "Comments on the losses : \n",
    "* Training loss decreases smoothly (down to 0.166), showing stable convergence.\n",
    "* Validation loss is noisy but remains reasonable\n",
    "* Growing train/val gap indicates a bit of overfitting but good overall generalization.\n",
    "* With the StepLR scheduler, the training loss decreases much more slowly and ends significantly higher (0.344), and the validation loss does not improve.\n",
    "* StepLR worsened performance because reducing LR too early prevents Adam from exploring the loss landscape.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fed2337",
   "metadata": {},
   "source": [
    "#### 2.1.2 - Binary Classification : CNN, Without augmentation, With Class Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b6213",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train (%)\")\n",
    "print(train_df[\"species_name\"].value_counts(normalize=True) * 100, \"\\n\")\n",
    "\n",
    "print(\"Val (%)\")\n",
    "print(val_df[\"species_name\"].value_counts(normalize=True) * 100, \"\\n\")\n",
    "\n",
    "print(\"Test (%)\")\n",
    "print(test_df[\"species_name\"].value_counts(normalize=True) * 100, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c893d674",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10f224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\") # mps is for macs with Apple Silicon\n",
    "\n",
    "model_cnn_noaug_classweight = CNN_V1().to(device)\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.array([0,1]),\n",
    "    y=train_df[\"species\"].values\n",
    ")\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model_cnn_noaug_classweight.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "# we don't use a lr schedulre here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a8472",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn_noaug_classweight = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "}\n",
    "\n",
    "for epoch in range(3):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model_cnn_noaug_classweight,\n",
    "        train_loader_binary_clf,\n",
    "        optimizer, criterion, device\n",
    "    )\n",
    "    \n",
    "    val_loss, val_acc, y_true, y_prob = evaluate(\n",
    "        model_cnn_noaug_classweight,\n",
    "        val_loader_binary_clf,\n",
    "        criterion, device\n",
    "    )\n",
    "\n",
    "    #scheduler.step() #we don't use a lr scheduler here\n",
    "\n",
    "    history_cnn_noaug_classweight[\"train_loss\"].append(train_loss)\n",
    "    history_cnn_noaug_classweight[\"train_acc\"].append(train_acc)\n",
    "    history_cnn_noaug_classweight[\"val_loss\"].append(val_loss)\n",
    "    history_cnn_noaug_classweight[\"val_acc\"].append(val_acc)\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.3f} | \"\n",
    "          f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.3f}\")\n",
    "\n",
    "# Final eval, after training\n",
    "val_loss, val_acc, last_y_true, last_y_prob = evaluate(\n",
    "        model_cnn_noaug_classweight,\n",
    "        val_loader_binary_clf,\n",
    "        criterion, device\n",
    "    )\n",
    "\n",
    "y_true, y_prob = last_y_true, last_y_prob\n",
    "metrics_cnn_noaug_classweight = classification_metrics(y_true,y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76b88b5",
   "metadata": {},
   "source": [
    "##### Save model weights, training info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_and_results(\n",
    "    save_dir=\"models/cnn_binary_noaug_cw_test3epoch\",\n",
    "    model=model_cnn_noaug_classweight,\n",
    "    history=history_cnn_noaug_classweight,\n",
    "    y_true=y_true,\n",
    "    y_prob=y_prob,\n",
    "    metrics=metrics_cnn_noaug_classweight,\n",
    "    model_filename=\"cnn_binary_noaug_cw_test3epoch.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d8f722",
   "metadata": {},
   "source": [
    "##### Results (Losses & Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25fafe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_noaug_classweight, history_cnn_noaug_classweight_df, y_true, y_prob, cm, metrics_cnn_noaug_classweight_json = load_model_and_results(\n",
    "    save_dir=\"models/cnn_binary_noaug_cw_test3epoch\",\n",
    "    model=model_cnn_noaug_classweight,\n",
    "    device=device,\n",
    "    model_filename=\"cnn_binary_noaug_cw_test3epoch.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6ec51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n -- Classification report --\")\n",
    "print(metrics_cnn_noaug_classweight_json[\"classification_report\"])\n",
    "print(\"ROC AUC:\", metrics_cnn_noaug_classweight_json[\"roc_auc\"])\n",
    "print(\"AP (PR AUC):\", metrics_cnn_noaug_classweight_json[\"average_precision\"])\n",
    "\n",
    "plot_train_val_loss(history_cnn_noaug_classweight_df)\n",
    "plot_accuracy(history_cnn_noaug_classweight_df)\n",
    "plot_confusion_matrix(cm,(\"Cat\",\"Dog\"))\n",
    "plot_roc_curve(y_true, y_prob, title=\"ROC Curve – CNN no aug, CEloss with class_weights\")\n",
    "plot_pr_curve(y_true, y_prob, title=\"PR Curve – CNN no aug, CEloss with class_weights\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef29a62e",
   "metadata": {},
   "source": [
    "##### Results (Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba230f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the val set\n",
    "show_random_classif_predictions(model_cnn_noaug_classweight, val_binary_clf, device, class_names=binary_target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "show_random_classif_predictions(model_cnn_noaug_classweight, test_binary_clf, device, class_names=binary_target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef1b1a1",
   "metadata": {},
   "source": [
    "| Task | Criterion | Optimizer | LR Scheduler | Model | Augmentation | Epochs | Train Loss | Val Loss | Train Acc | Val Acc | ROC AUC | PR AUC (AP) | Precision Cat | Recall Cat | Precision Dog | Recall Dog | F1-Score Cat | F1-Score Dog |Macro Precision | Macro Recall | Macro F1 |\n",
    "|------|-----------|-----------|--------|-----------|-----------|--------------|--------|------------|----------|-----------|----------|----------|--------------|----------------|-------------|----------------|-------------|---------------|----------------| ----------------| ----------------|\n",
    "| Binary Clf | CrossEntropyLoss (with balanced weights) | Adam (lr=1e-4, wd=1e-4) | No | CNN_V1 | None | 30 | 0.2015 | 1.5069 | 0.922 | 0.755 | 0.899 | 0.952 | 0.939 | 0.206 | 0.742 | 0.994 | 0.338 | 0.850 |  0.840 | 0.600 | 0.594 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf056f6",
   "metadata": {},
   "source": [
    "Cats misclassified as Dogs (non exhaustive list) :\n",
    "* Maine_Coon_157 (test set)\n",
    "* Siamese_56 (val set)\n",
    "* Siamese_232 (test set)\n",
    "* British_Shorthair_173 (val set)\n",
    "* British_Shorthair_166 (test set)\n",
    "* Bengal_79 (val set)\n",
    "* Egyptian_Mau_80 (test set)\n",
    "\n",
    "Dogs misclassified as Cats (non exhaustive list) :\n",
    "* NaN\n",
    "\n",
    "Comments on the metrics\n",
    "- Val. accuracy : 75.5% → significantly lower than the unweighted baseline (84.8%).\n",
    "- Macro F1 : 0.594 → much lower than the baseline 0.828 → the model performs unevenly across classes.\n",
    "- ROC AUC : 0.899 → decent but below the unweighted baseline (0.924), indicating weaker class separability.\n",
    "- PR AUC (AP) : 0.952 → still high, but slightly worse than the unweighted result (0.964).\n",
    "\n",
    "- Cat/Dog Precision : 0.939 vs 0.742 : Cat predictions are very accurate, but the model rarely predicts Cat at all.Dog precision decreases (0.926 → 0.742), meaning more false positives for Dogs.\n",
    "- Cat/Dog Recall : 0.206 vs 0.994 : Cat recall collapses (only 20% of Cats are detected). Dog recall becomes almost perfect, showing extreme prediction bias toward Dog.\n",
    "- Macro Precision / Recall / F1 : 0.840 / 0.600 / 0.594 : High macro precision but low macro recall → the classifier is extremely conservative on the Cat class.\n",
    "\n",
    "- Confusion matrix insight : Despite using balanced class weights, the model becomes more biased toward predicting Dog, not less.\n",
    "\n",
    "\n",
    "Comments on the Losses\n",
    "\n",
    "- Training loss decreases smoothly (down to 0.2015), optimization is stable.\n",
    "- Validation loss increases and is highly unstable (ends at 1.5069) : much worse than in the unweighted run (0.3793).\n",
    "- A strong train/val loss gap emerges, it indicates miscalibration and overfitting to the weighted objective.\n",
    "- Compared to the baseline, all major metrics degrade: accuracy, ROC AUC, Cat recall, macro F1, calibration, and stability.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "**Given the moderate imbalance (≈65% Dog / 35% Cat), weighted CrossEntropyLoss did not improve classification metrics and even degraded Cat recall. Since imbalance is not the main bottleneck, our next step is to introduce data augmentation to improve generalization.**.\n",
    "\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78615ff7",
   "metadata": {},
   "source": [
    "### 2.2 - Binary Classification : CNN, With augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005f32b6",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4321692",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\") # mps is for macs with Apple Silicon\n",
    "\n",
    "model_cnn_augmented = CNN_V1().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_cnn_augmented.parameters(), lr=1e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e359958",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn_augmented = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "}\n",
    "\n",
    "for epoch in range(30):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model_cnn_augmented,\n",
    "        train_loader_binary_clf_aug,\n",
    "        optimizer, criterion, device\n",
    "    )\n",
    "    \n",
    "    val_loss, val_acc, y_true, y_prob = evaluate(\n",
    "        model_cnn_augmented,\n",
    "        val_loader_binary_clf,\n",
    "        criterion, device\n",
    "    )\n",
    "\n",
    "    #scheduler.step()\n",
    "\n",
    "    history_cnn_augmented[\"train_loss\"].append(train_loss)\n",
    "    history_cnn_augmented[\"train_acc\"].append(train_acc)\n",
    "    history_cnn_augmented[\"val_loss\"].append(val_loss)\n",
    "    history_cnn_augmented[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.3f} | \"\n",
    "          f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.3f}\")\n",
    "\n",
    "# Final eval, after training\n",
    "val_loss, val_acc, last_y_true, last_y_prob = evaluate(\n",
    "        model_cnn_augmented,\n",
    "        val_loader_binary_clf,\n",
    "        criterion, device\n",
    "    )\n",
    "\n",
    "y_true, y_prob = last_y_true, last_y_prob\n",
    "metrics_cnn_augmented = classification_metrics(y_true,y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f9d7bc",
   "metadata": {},
   "source": [
    "##### Save model weights, training info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_and_results(\n",
    "    save_dir=\"models/cnn_binary_aug_30epoch\",\n",
    "    model=model_cnn_augmented,\n",
    "    history=history_cnn_augmented,\n",
    "    y_true=y_true,\n",
    "    y_prob=y_prob,\n",
    "    metrics=metrics_cnn_augmented,\n",
    "    model_filename=\"cnn_binary_aug_30epoch.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65ac3fd",
   "metadata": {},
   "source": [
    "##### Results (Losses & Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247129cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_augmented, history_cnn_augmented_df, y_true, y_prob, cm, metrics_cnn_augmented_json = load_model_and_results(\n",
    "    save_dir=\"models/cnn_binary_aug_30epoch\",\n",
    "    model=model_cnn_augmented,\n",
    "    device=device,\n",
    "    model_filename=\"cnn_binary_aug_30epoch.pth\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2497df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n -- Classification report --\")\n",
    "print(metrics_cnn_augmented_json[\"classification_report\"])\n",
    "print(\"ROC AUC:\", metrics_cnn_augmented_json[\"roc_auc\"])\n",
    "print(\"AP (PR AUC):\", metrics_cnn_augmented_json[\"average_precision\"])\n",
    "\n",
    "plot_train_val_loss(history_cnn_augmented_df)\n",
    "plot_accuracy(history_cnn_augmented_df)\n",
    "plot_confusion_matrix(cm, (\"Cat\", \"Dog\"))\n",
    "plot_roc_curve(y_true, y_prob)\n",
    "plot_pr_curve(y_true, y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d399c6a",
   "metadata": {},
   "source": [
    "##### Results (Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea5af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the val set\n",
    "show_random_classif_predictions(model_cnn_augmented,val_binary_clf, device, class_names=binary_target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a33d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "show_random_classif_predictions(model_cnn_noaug_classweight, test_binary_clf, device, class_names=binary_target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe960b84",
   "metadata": {},
   "source": [
    "| Task | Criterion | Optimizer | LR Scheduler | Model | Augmentation | Epochs | Train Loss | Val Loss | Train Acc | Val Acc | ROC AUC | PR AUC (AP) | Precision Cat | Recall Cat | Precision Dog | Recall Dog | F1-Score Cat | F1-Score Dog |Macro Precision | Macro Recall | Macro F1 |\n",
    "|------|-----------|-----------|--------|-----------|-----------|--------------|--------|------------|----------|-----------|----------|----------|--------------|----------------|-------------|----------------|-------------|---------------|----------------| ----------------| ----------------|\n",
    "| Binary Clf | CrossEntropyLoss | Adam (lr=1e-4, wd=1e-4) | No | CNN_V1 | Yes | 30 | 0.2865 | 0.3123 | 0.880 | 0.860 | 0.930 | 0.969 | 0.853 | 0.650 | 0.862 | 0.951 | 0.738 | 0.904 |  0.857 | 0.801 | 0.821 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d12fb",
   "metadata": {},
   "source": [
    "### 2.3 - Binary Classification with a Pre-Trained ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824f15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18_Binary(nn.Module):\n",
    "    def __init__(self,pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if pretrained:\n",
    "            weights = ResNet18_Weights.IMAGENET1K_V1\n",
    "            self.backbone = resnet18(weights=weights)\n",
    "        else :\n",
    "            self.backbone = resnet18(weights=None)\n",
    "\n",
    "        # we replace ResNet's final fully connected layer by a binary classif. head\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Linear(in_features,256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256,2)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.backbone(x)\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        \"\"\"\n",
    "            to freeze all backbone layers except the classification head\n",
    "        \"\"\"\n",
    "        for name, p in self.backbone.named_parameters():\n",
    "            if not name.startswith(\"fc.\"): #to keep head trainable\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def unfreeze_backbone(self):\n",
    "        \"\"\"\n",
    "            to unfreeze the backbone\n",
    "        \"\"\"\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = True\n",
    "    \n",
    "    def trainable_parameters(self):\n",
    "        \"\"\"\n",
    "            returns the parameters that require gradients\n",
    "        \"\"\"\n",
    "        return filter(lambda p: p.requires_grad, self.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d4abc5",
   "metadata": {},
   "source": [
    "#### 2.3.1 - Binary Classification : Pre-Trained CNN, Without augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047da3f6",
   "metadata": {},
   "source": [
    "- ResNet18 + Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80847d",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdac0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1 : Train head only (frozen backbone))\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\") # mps is for macs with Apple Silicon\n",
    "\n",
    "model_resnet18_noaug = ResNet18_Binary(pretrained=True).to(device)\n",
    "model_resnet18_noaug.freeze_backbone()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# we only optimize parameters that require grad (i.e. the head)\n",
    "optimizer_stage1 = torch.optim.AdamW(\n",
    "    model_resnet18_noaug.trainable_parameters(),\n",
    "    lr=1e-4,          \n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "history_resnet18_noaug= {\n",
    "    \"stage\": [],\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "}\n",
    "\n",
    "epochs_stage1 = 5  # (note : we need to keep it small : 3–5)\n",
    "\n",
    "for epoch in range(epochs_stage1):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model_resnet18_noaug,\n",
    "        train_loader_binary_clf,\n",
    "        optimizer_stage1, criterion, device\n",
    "    )\n",
    "\n",
    "    val_loss, val_acc, y_true, y_prob = evaluate(\n",
    "        model_resnet18_noaug,\n",
    "        val_loader_binary_clf,\n",
    "        criterion, device\n",
    "    )\n",
    "\n",
    "    history_resnet18_noaug[\"stage\"].append(1)\n",
    "    history_resnet18_noaug[\"epoch\"].append(epoch + 1)\n",
    "    history_resnet18_noaug[\"train_loss\"].append(train_loss)\n",
    "    history_resnet18_noaug[\"train_acc\"].append(train_acc)\n",
    "    history_resnet18_noaug[\"val_loss\"].append(val_loss)\n",
    "    history_resnet18_noaug[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(f\"[Stage 1][Epoch {epoch+1}] | Train Loss={train_loss:.4f}, Train Acc={train_acc:.3f} | Val Loss={val_loss:.4f}, Val Acc={val_acc:.3f}\")\n",
    "\n",
    "## Note : after this step, the head has learned a cat/dog boundary, the backbone is still pure ImageNet\n",
    "## Next we unfreeze the backbone, and use smaller LR\n",
    "\n",
    "# Stage 2 : Fine-Tuning\n",
    "model_resnet18_noaug.unfreeze_backbone()\n",
    "\n",
    "backbone_params = []\n",
    "head_params = []\n",
    "\n",
    "for name, p in model_resnet18_noaug.backbone.named_parameters():\n",
    "    if not p.requires_grad:\n",
    "        continue\n",
    "    if name.startswith(\"fc.\"):\n",
    "        head_params.append(p)\n",
    "    else:\n",
    "        backbone_params.append(p)\n",
    "\n",
    "optimizer_stage2 = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": backbone_params, \"lr\": 1e-5}, #small LR for pre-trained layers\n",
    "        {\"params\": head_params,     \"lr\": 1e-4}, #larger for the head\n",
    "    ],\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "epochs_stage2 = 30\n",
    "\n",
    "for epoch in range(epochs_stage2):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model_resnet18_noaug,\n",
    "        train_loader_binary_clf,\n",
    "        optimizer_stage2, criterion, device\n",
    "    )\n",
    "\n",
    "    val_loss, val_acc, y_true, y_prob = evaluate(\n",
    "        model_resnet18_noaug,\n",
    "        val_loader_binary_clf,\n",
    "        criterion, device\n",
    "    )\n",
    "\n",
    "    history_resnet18_noaug[\"stage\"].append(2)\n",
    "    history_resnet18_noaug[\"epoch\"].append(epochs_stage1 + epoch + 1)\n",
    "    history_resnet18_noaug[\"train_loss\"].append(train_loss)\n",
    "    history_resnet18_noaug[\"train_acc\"].append(train_acc)\n",
    "    history_resnet18_noaug[\"val_loss\"].append(val_loss)\n",
    "    history_resnet18_noaug[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(f\"[Stage 2][Epoch {epochs_stage1 + epoch + 1}] | Train Loss={train_loss:.4f}, Train Acc={train_acc:.3f} | Val Loss={val_loss:.4f}, Val Acc={val_acc:.3f}\")\n",
    "\n",
    "# Final eval on validation (or test_loader_binary_clf if you want)\n",
    "val_loss, val_acc, last_y_true, last_y_prob = evaluate(\n",
    "    model_resnet18_noaug,\n",
    "    val_loader_binary_clf,\n",
    "    criterion, device\n",
    ")\n",
    "\n",
    "y_true, y_prob = last_y_true, last_y_prob\n",
    "metrics_resnet18_noaug = classification_metrics(y_true, y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389976bd",
   "metadata": {},
   "source": [
    "##### Save model weights, training info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24dc764",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_and_results(\n",
    "    save_dir=\"models/resnet18_binary_noaug\",\n",
    "    model=model_resnet18_noaug,\n",
    "    history=history_resnet18_noaug,\n",
    "    y_true=y_true,\n",
    "    y_prob=y_prob,\n",
    "    metrics=metrics_resnet18_noaug,\n",
    "    model_filename=\"resnet18_binary_noaug.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c4250b",
   "metadata": {},
   "source": [
    "##### Results (Losses & Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a46937",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet18_noaug, history_resnet18_noaug_df, y_true, y_prob, cm, metrics_resnet18_noaug_json = load_model_and_results(\n",
    "    save_dir=\"models/resnet18_binary_noaug\",\n",
    "    model=model_resnet18_noaug, \n",
    "    device=device,\n",
    "    model_filename=\"resnet18_binary_noaug.pth\",\n",
    ")\n",
    "\n",
    "print(\"\\n -- Classification report --\")\n",
    "print(metrics_resnet18_noaug_json[\"classification_report\"])\n",
    "print(\"ROC AUC:\", metrics_resnet18_noaug_json[\"roc_auc\"])\n",
    "print(\"AP (PR AUC):\", metrics_resnet18_noaug_json[\"average_precision\"])\n",
    "\n",
    "plot_train_val_loss(history_resnet18_noaug_df)\n",
    "plot_accuracy(history_resnet18_noaug_df)\n",
    "plot_confusion_matrix(cm, (\"Cat\", \"Dog\"))\n",
    "plot_roc_curve(y_true, y_prob, title=\"ROC Curve – ResNet18 (pretrained + finetuned)\")\n",
    "plot_pr_curve(y_true, y_prob, title=\"PR Curve – ResNet18 (pretrained + finetuned)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2f080",
   "metadata": {},
   "source": [
    "##### Results (Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a56a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the val set\n",
    "show_random_classif_predictions(model_resnet18_noaug, val_binary_clf, device, class_names=binary_target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f78eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "show_random_classif_predictions(model_resnet18_noaug, test_binary_clf, device, class_names=binary_target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e034f",
   "metadata": {},
   "source": [
    "| Task | Criterion | Optimizer | LR Scheduler | Model | Augmentation | Epochs | Train Loss | Val Loss | Train Acc | Val Acc | ROC AUC | PR AUC (AP) | Precision Cat | Recall Cat | Precision Dog | Recall Dog | F1-Score Cat | F1-Score Dog |Macro Precision | Macro Recall | Macro F1 |\n",
    "|------|-----------|-----------|--------|-----------|-----------|--------------|--------|------------|----------|-----------|----------|----------|--------------|----------------|-------------|----------------|-------------|---------------|----------------| ----------------| ----------------|\n",
    "| ResNet18 | CrossEntropyLoss | AdamW (Stage 1: head lr=1e-4; Stage 2: backbone=1e-5, head=1e-4) | No | ResNet18_Binary | No | 5 + 30 | 0.0001 | 0.0503 | 1.0 | 0.990 | 0.999 | 0.999 | 1.0 | 0.969 | 0.985 | 1.0 | 0.984  | 0.993 |  0.993 | 0.934 | 0.989 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce407ea2",
   "metadata": {},
   "source": [
    "#### 2.3.2 - Binary Classification : Pre-Trained CNN, With augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82051e5e",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21f59d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1 : Train head only (frozen backbone))\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\") # mps is for macs with Apple Silicon\n",
    "\n",
    "model_resnet18_aug = ResNet18_Binary(pretrained=True).to(device)\n",
    "model_resnet18_aug.freeze_backbone()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# we only optimize parameters that require grad (i.e. the head)\n",
    "optimizer_stage1 = torch.optim.AdamW(\n",
    "    model_resnet18_aug.trainable_parameters(),\n",
    "    lr=1e-4,          \n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "history_resnet18_aug= {\n",
    "    \"stage\": [],\n",
    "    \"epoch\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "}\n",
    "\n",
    "epochs_stage1 = 5  # (note : we need to keep it small : 3–5)\n",
    "\n",
    "for epoch in range(epochs_stage1):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model_resnet18_aug,\n",
    "        train_loader_binary_clf_aug,\n",
    "        optimizer_stage1, criterion, device\n",
    "    )\n",
    "\n",
    "    val_loss, val_acc, y_true, y_prob = evaluate(\n",
    "        model_resnet18_aug,\n",
    "        val_loader_binary_clf,\n",
    "        criterion, device\n",
    "    )\n",
    "\n",
    "    history_resnet18_aug[\"stage\"].append(1)\n",
    "    history_resnet18_aug[\"epoch\"].append(epoch + 1)\n",
    "    history_resnet18_aug[\"train_loss\"].append(train_loss)\n",
    "    history_resnet18_aug[\"train_acc\"].append(train_acc)\n",
    "    history_resnet18_aug[\"val_loss\"].append(val_loss)\n",
    "    history_resnet18_aug[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(f\"[Stage 1][Epoch {epoch+1}] | Train Loss={train_loss:.4f}, Train Acc={train_acc:.3f} | Val Loss={val_loss:.4f}, Val Acc={val_acc:.3f}\")\n",
    "\n",
    "## Note : after this step, the head has learned a cat/dog boundary, the backbone is still pure ImageNet\n",
    "## Next we unfreeze the backbone, and use smaller LR\n",
    "\n",
    "# Stage 2 : Fine-Tuning\n",
    "model_resnet18_aug.unfreeze_backbone()\n",
    "\n",
    "backbone_params = []\n",
    "head_params = []\n",
    "\n",
    "for name, p in model_resnet18_aug.backbone.named_parameters():\n",
    "    if not p.requires_grad:\n",
    "        continue\n",
    "    if name.startswith(\"fc.\"):\n",
    "        head_params.append(p)\n",
    "    else:\n",
    "        backbone_params.append(p)\n",
    "\n",
    "optimizer_stage2 = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": backbone_params, \"lr\": 1e-5}, #small LR for pre-trained layers\n",
    "        {\"params\": head_params,     \"lr\": 1e-4}, #larger for the head\n",
    "    ],\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "\n",
    "epochs_stage2 = 30\n",
    "\n",
    "for epoch in range(epochs_stage2):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model_resnet18_aug,\n",
    "        train_loader_binary_clf_aug,\n",
    "        optimizer_stage2, criterion, device\n",
    "    )\n",
    "\n",
    "    val_loss, val_acc, y_true, y_prob = evaluate(\n",
    "        model_resnet18_aug,\n",
    "        val_loader_binary_clf,\n",
    "        criterion, device\n",
    "    )\n",
    "\n",
    "    history_resnet18_aug[\"stage\"].append(2)\n",
    "    history_resnet18_aug[\"epoch\"].append(epochs_stage1 + epoch + 1)\n",
    "    history_resnet18_aug[\"train_loss\"].append(train_loss)\n",
    "    history_resnet18_aug[\"train_acc\"].append(train_acc)\n",
    "    history_resnet18_aug[\"val_loss\"].append(val_loss)\n",
    "    history_resnet18_aug[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(f\"[Stage 2][Epoch {epochs_stage1 + epoch + 1}] | Train Loss={train_loss:.4f}, Train Acc={train_acc:.3f} | Val Loss={val_loss:.4f}, Val Acc={val_acc:.3f}\")\n",
    "\n",
    "# Final eval on validation (or test_loader_binary_clf if you want)\n",
    "val_loss, val_acc, last_y_true, last_y_prob = evaluate(\n",
    "    model_resnet18_aug,\n",
    "    val_loader_binary_clf,\n",
    "    criterion, device\n",
    ")\n",
    "\n",
    "y_true, y_prob = last_y_true, last_y_prob\n",
    "metrics_resnet18_aug = classification_metrics(y_true, y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340b7491",
   "metadata": {},
   "source": [
    "##### Save model weights, training info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd146dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_and_results(\n",
    "    save_dir=\"models/resnet18_binary_aug_5p30epochs\",\n",
    "    model=model_resnet18_aug,\n",
    "    history=history_resnet18_aug,\n",
    "    y_true=y_true,\n",
    "    y_prob=y_prob,\n",
    "    metrics=metrics_resnet18_aug,\n",
    "    model_filename=\"resnet18_binary_aug_5p30epochs.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90f0e1",
   "metadata": {},
   "source": [
    "##### Results (Losses & Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b6abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet18_aug, history_resnet18_aug_df, y_true, y_prob, cm, metrics_resnet18_aug_json = load_model_and_results(\n",
    "    save_dir=\"models/resnet18_binary_aug_5p30epochs\",\n",
    "    model=model_resnet18_aug, \n",
    "    device=device,\n",
    "    model_filename=\"resnet18_binary_aug_5p30epochs.pth\",\n",
    ")\n",
    "\n",
    "print(\"\\n -- Classification report --\")\n",
    "print(metrics_resnet18_aug_json[\"classification_report\"])\n",
    "print(\"ROC AUC:\", metrics_resnet18_aug_json[\"roc_auc\"])\n",
    "print(\"AP (PR AUC):\", metrics_resnet18_aug_json[\"average_precision\"])\n",
    "\n",
    "plot_train_val_loss(history_resnet18_aug_df)\n",
    "plot_accuracy(history_resnet18_aug_df)\n",
    "plot_confusion_matrix(cm, (\"Cat\", \"Dog\"))\n",
    "plot_roc_curve(y_true, y_prob, title=\"ROC Curve – ResNet18 (pretrained + finetuned) + augmentation\")\n",
    "plot_pr_curve(y_true, y_prob, title=\"PR Curve – ResNet18 (pretrained + finetuned) + augmentation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21ba8f",
   "metadata": {},
   "source": [
    "##### Results (Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b859ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the val set\n",
    "show_random_classif_predictions(model_resnet18_aug, val_binary_clf, device, class_names=binary_target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3663330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "show_random_classif_predictions(model_resnet18_aug, test_binary_clf, device, class_names=binary_target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b25b05",
   "metadata": {},
   "source": [
    "| Task | Criterion | Optimizer | LR Scheduler | Model | Augmentation | Epochs | Train Loss | Val Loss | Train Acc | Val Acc | ROC AUC | PR AUC (AP) | Precision Cat | Recall Cat | Precision Dog | Recall Dog | F1-Score Cat | F1-Score Dog |Macro Precision | Macro Recall | Macro F1 |\n",
    "|------|-----------|-----------|--------|-----------|-----------|--------------|--------|------------|----------|-----------|----------|----------|--------------|----------------|-------------|----------------|-------------|---------------|----------------| ----------------| ----------------|\n",
    "| ResNet18 | CrossEntropyLoss | AdamW (Stage 1: head lr=1e-4; Stage 2: backbone=1e-5, head=1e-4) | No | ResNet18_Binary | Yes | 5 + 30 | 0.0021 | 0.0352 | 0.999 | 0.990 | 0.999 | 1.0 | 0.995 | 0.973 | 0.988 | 0.998 | 0.984 | 0.993 | 0.992  | 0.986 |  0.980 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df582b92",
   "metadata": {},
   "source": [
    "### 2.5 - Conclusion on Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909b3fcd",
   "metadata": {},
   "source": [
    "| Task | Criterion | Optimizer | LR Scheduler | Model | Augmentation | Epochs | Train Loss | Val Loss | Train Acc | Val Acc | ROC AUC | PR AUC (AP) | Precision Cat | Recall Cat | Precision Dog | Recall Dog | F1-Score Cat | F1-Score Dog |Macro Precision | Macro Recall | Macro F1 |\n",
    "|------|-----------|-----------|--------|-----------|-----------|--------------|--------|------------|----------|-----------|----------|----------|--------------|----------------|-------------|----------------|-------------|---------------|----------------| ----------------| ----------------|\n",
    "| Binary Clf | CrossEntropyLoss | Adam (lr=1e-4, wd=1e-4) | No | CNN_V1 | None | 30 | 0.1661 | 0.3793 | 0.933 | 0.848 | 0.924 | 0.963 | 0.709 | 0.843 | 0.926 | 0.850 | 0.770 | 0.886 | 0.817 | 0.846 | 0.828 |\n",
    "| Binary Clf | CrossEntropyLoss | Adam (lr=1e-4, wd=1e-4) | step_size=10, gamma = 0.1 | CNN_V1 | None | 30 | 0.3440 | 0.4041 | 0.856 | 0.822 | 0.871 | 0.939 | 0.728 | 0.659 | 0.857 | 0.893 | 0.692 | 0.875 | NA | NA | NA |\n",
    "| Binary Clf | CrossEntropyLoss (with balanced weights) | Adam (lr=1e-4, wd=1e-4) | No | CNN_V1 | None | 30 | 0.2015 | 1.5069 | 0.922 | 0.755 | 0.899 | 0.952 | 0.939 | 0.206 | 0.742 | 0.994 | 0.338 | 0.850 |  0.840 | 0.600 | 0.594 |\n",
    "| Binary Clf | CrossEntropyLoss | Adam (lr=1e-4, wd=1e-4) | No | CNN_V1 | Yes | 30 | 0.2865 | 0.3123 | 0.880 | 0.860 | 0.930 | 0.969 | 0.853 | 0.650 | 0.862 | 0.951 | 0.738 | 0.904 |  0.857 | 0.801 | 0.821 |\n",
    "| ResNet18 | CrossEntropyLoss | AdamW (Stage 1: head lr=1e-4; Stage 2: backbone=1e-5, head=1e-4) | No | ResNet18_Binary | No | 5 + 30 | 0.0001 | 0.0503 | 1.0 | 0.990 | 0.999 | 0.999 | 1.0 | 0.969 | 0.985 | 1.0 | 0.984  | 0.993 |  0.993 | 0.934 | 0.989 |\n",
    "| ResNet18 | CrossEntropyLoss | AdamW (Stage 1: head lr=1e-4; Stage 2: backbone=1e-5, head=1e-4) | No | ResNet18_Binary | Yes | 5 + 30 | 0.0021 | 0.0352 | 0.999 | 0.990 | 0.999 | 1.0 | 0.995 | 0.973 | 0.988 | 0.998 | 0.984 | 0.993 | 0.992  | 0.986 |  0.980 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8755cc82",
   "metadata": {},
   "source": [
    "## 3 - Multiclass (Breed) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff22cff5",
   "metadata": {},
   "source": [
    "### 3.0 Functions for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, img_path, transform, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor)\n",
    "        pred = logits.argmax(dim=1).item()\n",
    "\n",
    "    return pred, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dadcf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def study_most_confused_breeds(cm, label_map, top_k=10):\n",
    "\n",
    "    # Normalize rows \n",
    "    cm_normalized = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    pairs = []\n",
    "    num_classes = len(cm_normalized)\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            if i != j:\n",
    "                true_label = label_map[i] if not isinstance(label_map, dict) else label_map[i]\n",
    "                pred_label = label_map[j] if not isinstance(label_map, dict) else label_map[j]\n",
    "                pairs.append((true_label, pred_label, cm_normalized[i, j]))\n",
    "\n",
    "    # Sorts confusion\n",
    "    confusion_pairs = sorted(pairs, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Print of most confused pairs\n",
    "    print(f\"\\nTop {top_k} most confused class pairs:\")\n",
    "    for true_label, pred_label, rate in confusion_pairs[:top_k]:\n",
    "        print(f\"{true_label} → {pred_label}: {rate:.2f}\")\n",
    "\n",
    "    return confusion_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb99003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_misclassified_pair(\n",
    "    model,\n",
    "    samples,\n",
    "    true_label_idx,\n",
    "    pred_label_idx,\n",
    "    label_map,\n",
    "    transform,\n",
    "    device,\n",
    "    n_samples=5,\n",
    "    label_col=\"breed_idx\",   # <-- default: your multiclass label column\n",
    "    path_col=\"img_path\",     # <-- in case this differs too\n",
    "):\n",
    "    model.eval()\n",
    "    misclassified = []\n",
    "\n",
    "    for _, row in samples.iterrows():\n",
    "        img_path = row[path_col]\n",
    "        pred_idx, img = predict_image(model, img_path, transform, device)\n",
    "        true_idx = row[label_col]\n",
    "\n",
    "        if true_idx == true_label_idx and pred_idx == pred_label_idx:\n",
    "            misclassified.append((img, true_idx, pred_idx))\n",
    "\n",
    "    if len(misclassified) == 0:\n",
    "        print(\"No misclassified samples found for this pair.\")\n",
    "        return\n",
    "\n",
    "    examples = random.sample(misclassified, min(n_samples, len(misclassified)))\n",
    "\n",
    "    n_cols = len(examples)\n",
    "    plt.figure(figsize=(4 * n_cols, 4))\n",
    "    for i, (img, true_idx, pred_idx) in enumerate(examples):\n",
    "        plt.subplot(1, n_cols, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"True: {label_map[true_idx]}\\nPred: {label_map[pred_idx]}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff1d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_class_names = (\n",
    "    \"Abyssinian\", \"american_bulldog\", \"american_pit_bull_terrier\", \"basset_hound\",\n",
    "    \"beagle\", \"Bengal\", \"Birman\", \"bombay\", \"boxer\", \"British_Shorthair\",\n",
    "    \"chihuahua\", \"Egyptian_Mau\", \"english_cocker_spaniel\", \"english_setter\",\n",
    "    \"german_shorthaired\", \"great_pyrenees\", \"havanese\", \"japanese_chin\",\n",
    "    \"keeshond\", \"leonberger\", \"Maine_Coon\", \"miniature_pinscher\", \"newfoundland\",\n",
    "    \"Persian\", \"pomeranian\", \"pug\", \"Ragdoll\", \"Russian_Blue\", \"samoyed\",\"saint_bernard\",\n",
    "    \"scottish_terrier\", \"shiba_inu\", \"Siamese\", \"Sphynx\", \"staffordshire_bull_terrier\",\n",
    "    \"wheaten_terrier\", \"yorkshire_terrier\"\n",
    ")\n",
    "\n",
    "multiclass_target_names = list(multiclass_class_names)\n",
    "print(len(multiclass_class_names))\n",
    "\n",
    "label_map_breed = {i: name for i, name in enumerate(multiclass_class_names)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60377a14",
   "metadata": {},
   "source": [
    "### 3.1 Multiclass classification: CNN, without augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa371e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_multiclass(nn.Module):\n",
    "    def __init__(self, num_classes=37):\n",
    "        super(CNN_multiclass, self).__init__()\n",
    "        \n",
    "        # Conv blocks matching Keras architecture\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Input: 224x224x3\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # Output: 224x224x64\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)                          # Output: 112x112x64\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # Output: 112x112x128\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # Output: 112x112x256\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)                          # Output: 56x56x256\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        # Calculate flatten size: 56x56x256 = 802816\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(56 * 56 * 256, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv block 1 (matching Keras Conv2D(32) + Conv2D(64) + MaxPool)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Conv block 2 (enhanced for color images)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # FC layers (matching Keras Dense(128) + Dense(num_classes))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Raw logits (softmax in loss)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20923cfb",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e0a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model_CNN_multiclass = CNN_multiclass().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_CNN_multiclass.parameters(), lr=1e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d53cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_multiclass = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "}\n",
    "\n",
    "for epoch in range(3):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model_CNN_multiclass,\n",
    "        train_loader_multi_clf,\n",
    "        optimizer, criterion, device\n",
    "    )\n",
    "    \n",
    "    val_loss, val_acc, y_true, y_prob = evaluate_multiclass(\n",
    "        model_CNN_multiclass,\n",
    "        val_loader_multi_clf,\n",
    "        criterion, device\n",
    "    )\n",
    "    history_multiclass[\"train_loss\"].append(train_loss)\n",
    "    history_multiclass[\"train_acc\"].append(train_acc)\n",
    "    history_multiclass[\"val_loss\"].append(val_loss)\n",
    "    history_multiclass[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.3f} | \"\n",
    "          f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.3f}\")\n",
    "\n",
    "# Final eval, after training\n",
    "val_loss, val_acc, last_y_true, last_y_prob = evaluate_multiclass(\n",
    "        model_CNN_multiclass,\n",
    "        val_loader_multi_clf,\n",
    "        criterion, device\n",
    "    )\n",
    "\n",
    "y_true, y_prob = last_y_true, last_y_prob\n",
    "metrics_multiclass = classification_metrics_multiclass(y_true,y_prob,class_names=multiclass_target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00753cd0",
   "metadata": {},
   "source": [
    "##### Save model weights, training info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b81275",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_and_results(\n",
    "    save_dir=\"models/cnn_multiclass_noaug_test3epochs\",\n",
    "    model=model_CNN_multiclass,\n",
    "    history=history_multiclass,\n",
    "    y_true=y_true,\n",
    "    y_prob=y_prob,\n",
    "    metrics=metrics_multiclass,\n",
    "    model_filename=\"cnn_multiclass_noaug_test3epochs.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6513876",
   "metadata": {},
   "source": [
    "##### Results (Losses & Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfcc34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN_multiclass, history_multiclass, y_true_multiclass, y_prob_multiclass, cm_multiclass, metrics_multiclass = load_model_and_results(\n",
    "    save_dir=\"models/cnn_multiclass_noaug_test3epochs\",\n",
    "    model=model_CNN_multiclass, \n",
    "    device=device,\n",
    "    model_filename=\"cnn_multiclass_noaug_test3epochs.pth\",\n",
    ")\n",
    "\n",
    "print(\"\\n -- Classification report --\")\n",
    "print(metrics_multiclass[\"classification_report\"])\n",
    "print(\"ROC AUC:\", metrics_multiclass[\"roc_auc\"])\n",
    "print(\"AP (PR AUC):\", metrics_multiclass[\"average_precision\"])\n",
    "\n",
    "plot_train_val_loss(history_multiclass)\n",
    "plot_accuracy(history_multiclass)\n",
    "plot_confusion_matrix(cm_multiclass, multiclass_class_names)\n",
    "\n",
    "\n",
    "roc_auc_macro_ovr = roc_auc_score(y_true, y_prob, multi_class=\"ovr\", average=\"macro\")\n",
    "#roc_auc_micro_ovr = roc_auc_score(y_true, y_prob, multi_class=\"ovr\", average=\"micro\")\n",
    "roc_auc_macro_ovo = roc_auc_score(y_true, y_prob, multi_class=\"ovo\", average=\"macro\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886d24f9",
   "metadata": {},
   "source": [
    "##### Results (Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a7a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_pairs = study_most_confused_breeds(cm = cm_multiclass, label_map = label_map_breed, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6fbe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most confused pair (names)\n",
    "true_label_name, pred_label_name, _ = confusion_pairs[0]\n",
    "\n",
    "# name -> idx\n",
    "name_to_idx = {name: idx for idx, name in label_map_breed.items()}\n",
    "true_idx = name_to_idx[true_label_name]\n",
    "pred_idx = name_to_idx[pred_label_name]\n",
    "\n",
    "# visualize misclassifications on the validation set\n",
    "plot_misclassified_pair(\n",
    "    model_CNN_multiclass,\n",
    "    val_df,                 # DataFrame with img paths + labels\n",
    "    true_idx,\n",
    "    pred_idx,\n",
    "    label_map_breed,\n",
    "    val_transform,\n",
    "    device=device,\n",
    "    n_samples=5,\n",
    "    label_col=\"id\",  # <-- important: use your real column name\n",
    "    path_col=\"img_path\",    # adjust if your column is named differently\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e2b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the val set\n",
    "show_random_classif_predictions_topk(\n",
    "    model_CNN_multiclass,\n",
    "    val_multi_clf,         \n",
    "    device,\n",
    "    class_names=multiclass_target_names,\n",
    "    num_images=9,\n",
    "    k=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d960a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "show_random_classif_predictions_topk(\n",
    "    model_CNN_multiclass,\n",
    "    test_multi_clf,         \n",
    "    device,\n",
    "    class_names=multiclass_target_names,\n",
    "    num_images=9,\n",
    "    k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0472577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To do : Train on 30 epochs and Add table with results (note some metrics are not relevant for multi-class (ROC_AUC curve / PR_Curve, maybe some others))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07a028a",
   "metadata": {},
   "source": [
    "### 3.2 Multiclass classification: CNN, with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087ea441",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model_CNN_multiclass_aug = CNN_multiclass().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_CNN_multiclass_aug.parameters(), lr=1e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f9928",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_multiclass_aug = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "}\n",
    "\n",
    "for epoch in range(3):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model_CNN_multiclass_aug,\n",
    "        train_loader_multi_clf_aug,\n",
    "        optimizer, criterion, device\n",
    "    )\n",
    "    \n",
    "    val_loss, val_acc, y_true, y_prob = evaluate_multiclass(\n",
    "        model_CNN_multiclass_aug,\n",
    "        val_loader_multi_clf,\n",
    "        criterion, device\n",
    "    )\n",
    "    history_multiclass_aug[\"train_loss\"].append(train_loss)\n",
    "    history_multiclass_aug[\"train_acc\"].append(train_acc)\n",
    "    history_multiclass_aug[\"val_loss\"].append(val_loss)\n",
    "    history_multiclass_aug[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.3f} | \"\n",
    "          f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.3f}\")\n",
    "\n",
    "# Final eval, after training\n",
    "val_loss, val_acc, last_y_true, last_y_prob = evaluate_multiclass(\n",
    "        model_CNN_multiclass_aug,\n",
    "        val_loader_multi_clf,\n",
    "        criterion, device\n",
    "    )\n",
    "\n",
    "y_true, y_prob = last_y_true, last_y_prob\n",
    "metrics_multiclass_aug = classification_metrics_multiclass(y_true,y_prob,class_names=multiclass_target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedc6caa",
   "metadata": {},
   "source": [
    "##### Save model weights, training info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9438a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_and_results(\n",
    "    save_dir=\"models/cnn_multiclass_aug_test3epochs\",\n",
    "    model=model_CNN_multiclass_aug,\n",
    "    history=history_multiclass_aug,\n",
    "    y_true=y_true,\n",
    "    y_prob=y_prob,\n",
    "    metrics=metrics_multiclass_aug,\n",
    "    model_filename=\"cnn_multiclass_aug_test3epochs.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec6f5a8",
   "metadata": {},
   "source": [
    "##### Results (Losses & Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59e149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN_multiclass_aug, history_multiclass_aug, y_true_multiclass, y_prob_multiclass, cm_multiclass_aug, metrics_multiclass_aug = load_model_and_results(\n",
    "    save_dir=\"models/cnn_multiclass_aug_test3epochs\",\n",
    "    model=model_CNN_multiclass_aug, \n",
    "    device=device,\n",
    "    model_filename=\"cnn_multiclass_aug_test3epochs.pth\",\n",
    ")\n",
    "\n",
    "print(\"\\n -- Classification report --\")\n",
    "print(metrics_multiclass_aug[\"classification_report\"])\n",
    "print(\"ROC AUC:\", metrics_multiclass_aug[\"roc_auc\"])\n",
    "print(\"AP (PR AUC):\", metrics_multiclass_aug[\"average_precision\"])\n",
    "\n",
    "plot_train_val_loss(history_multiclass_aug)\n",
    "plot_accuracy(history_multiclass_aug)\n",
    "plot_confusion_matrix(cm_multiclass_aug, multiclass_class_names)\n",
    "\n",
    "\n",
    "roc_auc_macro_ovr = roc_auc_score(y_true, y_prob, multi_class=\"ovr\", average=\"macro\")\n",
    "#roc_auc_micro_ovr = roc_auc_score(y_true, y_prob, multi_class=\"ovr\", average=\"micro\")\n",
    "roc_auc_macro_ovo = roc_auc_score(y_true, y_prob, multi_class=\"ovo\", average=\"macro\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fc605e",
   "metadata": {},
   "source": [
    "##### Results (Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e68f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_pairs_aug = study_most_confused_breeds(cm = cm_multiclass_aug, label_map = label_map_breed, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba462b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most confused pair (names)\n",
    "true_label_name, pred_label_name, _ = confusion_pairs_aug[0]\n",
    "\n",
    "# name -> idx\n",
    "name_to_idx = {name: idx for idx, name in label_map_breed.items()}\n",
    "true_idx = name_to_idx[true_label_name]\n",
    "pred_idx = name_to_idx[pred_label_name]\n",
    "\n",
    "# visualize misclassifications on the validation set\n",
    "plot_misclassified_pair(\n",
    "    model_CNN_multiclass_aug,\n",
    "    val_df,                 # DataFrame with img paths + labels\n",
    "    true_idx,\n",
    "    pred_idx,\n",
    "    label_map_breed,\n",
    "    val_transform,\n",
    "    device=device,\n",
    "    n_samples=5,\n",
    "    label_col=\"id\",  \n",
    "    path_col=\"img_path\",  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b7bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the val set\n",
    "show_random_classif_predictions_topk(\n",
    "    model_CNN_multiclass_aug,\n",
    "    val_multi_clf,         \n",
    "    device,\n",
    "    class_names=multiclass_target_names,\n",
    "    num_images=9,\n",
    "    k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae3ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the test set\n",
    "show_random_classif_predictions_topk(\n",
    "    model_CNN_multiclass_aug,\n",
    "    test_multi_clf,         \n",
    "    device,\n",
    "    class_names=multiclass_target_names,\n",
    "    num_images=9,\n",
    "    k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891db7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### To do : Train on 30 epochs and Add table with results (note some metrics are not relevant for multi-class (ROC_AUC curve / PR_Curve, maybe some others))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf34a4a",
   "metadata": {},
   "source": [
    "-----\n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f075ac14",
   "metadata": {},
   "source": [
    "### 3.3 Multiclass classification: Pre-Trained CNN, without augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64cb808",
   "metadata": {},
   "source": [
    "When implementing a CNN from scratch, we did not achieve the best results. The Oxford-IIIT dataset is a small one, so learning from scratch does not necessarily give the best results. \n",
    "We will apply the vgg-16 model as our pre-trained model. VGG-16 is pre-trained on ImageNet, which contains many images similar to our dataset. We will freeze the features and only train the classifier head. This reduced the number of trainable parameters. We change the last linear layer to include 37 classes for the 37 breeds we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b29771",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_mod = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "for param in vgg16_mod.features.parameters():\n",
    "    param.requires_grad = False  \n",
    "vgg16_mod.classifier[6] = nn.Linear(4096, 37)\n",
    "vgg16_mod.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3ff833",
   "metadata": {},
   "source": [
    "CrossEntropyLoss combines log-softmax and negative log-likelihood loss. It penalizes confident wrong predictions and rewards confident correct ones. \n",
    "\n",
    "Adam: \n",
    "- Adaptive per-parameter learning rates with momentum work better than plain SGD for fine-tuning, especially with the non-convex loss landscape of the classifier layers. It converges faster and handles sparse gradients well.\n",
    "- Freezing features to prevent catastrophic forgetting of ImageNet features and reducing overfitting on 7k images. \n",
    "- Low learning rate to avoid destroying pretrained representations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a4d4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()        \n",
    "optimizer = torch.optim.Adam(vgg16_mod.classifier.parameters(),lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747a1369",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_vgg16 = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "}\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        vgg16_mod,\n",
    "        train_loader_multi_clf,\n",
    "        optimizer, criterion, device\n",
    "    )\n",
    "    \n",
    "    val_loss, val_acc, y_true, y_prob = evaluate(\n",
    "        vgg16_mod,\n",
    "        val_loader_multi_clf,\n",
    "        criterion, device\n",
    "    )\n",
    "    history_vgg16[\"train_loss\"].append(train_loss)\n",
    "    history_vgg16[\"train_acc\"].append(train_acc)\n",
    "    history_vgg16[\"val_loss\"].append(val_loss)\n",
    "    history_vgg16[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.3f} | \"\n",
    "          f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.3f}\")\n",
    "\n",
    "# Final eval, after training\n",
    "val_loss, val_acc, last_y_true, last_y_prob = evaluate(\n",
    "        vgg16_mod,\n",
    "        val_loader_multi_clf,\n",
    "        criterion, device\n",
    "    )\n",
    "y_true, y_prob = last_y_true, last_y_prob\n",
    "metrics_vgg16 = classification_metrics(y_true,y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbba94b0",
   "metadata": {},
   "source": [
    "##### Save model weights, training info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d555cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_and_results(\n",
    "    save_dir=\"vgg16\",\n",
    "    model=vgg16_mod,\n",
    "    history=history_vgg16,\n",
    "    y_true=y_true,\n",
    "    y_prob=y_prob,\n",
    "    metrics=metrics_vgg16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d510c0eb",
   "metadata": {},
   "source": [
    "##### Results (Losses & Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd777dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16, history_vgg16, y_true_vgg16, y_prob_vgg16, cm_vgg16, metrics_vgg16 = load_model_and_results(\n",
    "    save_dir=\"vgg16\",\n",
    "    #model=model_cnn_augmented,\n",
    "    model=lambda: vgg16_mod,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c01555",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n -- Classification report --\")\n",
    "print(metrics_vgg16[\"classification_report\"])\n",
    "print(\"ROC AUC:\", metrics_vgg16[\"roc_auc\"])\n",
    "print(\"AP (PR AUC):\", metrics_vgg16[\"average_precision\"])\n",
    "\n",
    "plot_train_val_loss(history_vgg16)\n",
    "plot_accuracy(history_vgg16)\n",
    "plot_confusion_matrix(cm_vgg16, multiclass_class_names)\n",
    "plot_roc_curve(y_true_vgg16, y_prob_vgg16)\n",
    "plot_pr_curve(y_true_vgg16, y_prob_vgg16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb53436",
   "metadata": {},
   "source": [
    "##### Results (Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff7513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the val set\n",
    "show_random_classif_predictions(model_vgg16,val_multi_clf, device, class_names=multiclass_target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e84189",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_pairs = study_most_confused_breeds(cm = cm_vgg16, label_map = label_map_breed, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f25286",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label_idx, pred_label_idx, _ = confusion_pairs[0] \n",
    "\n",
    "true_idx = label_map_breed.index(true_label_idx)  \n",
    "pred_idx = label_map_breed.index(pred_label_idx)\n",
    "\n",
    "# Plots examples of misclassified images\n",
    "plot_misclassified_pair(model_vgg16, confusion_pairs, true_idx, pred_idx,\n",
    "                        label_map_breed, train_transform, device=device, n_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c926f70e",
   "metadata": {},
   "source": [
    "### 3.4 Multiclass classification: Pre-Trained CNN, with augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b101c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()        \n",
    "optimizer = torch.optim.Adam(vgg16_mod.classifier.parameters(),lr=1e-4)\n",
    "\n",
    "vgg16_aug = vgg16_mod.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796d2649",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_vgg16_aug = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "}\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        vgg16_aug,\n",
    "        train_loader_multi_clf_aug,\n",
    "        optimizer, criterion, device\n",
    "    )\n",
    "    \n",
    "    val_loss, val_acc, y_true, y_prob = evaluate(\n",
    "        vgg16_aug,\n",
    "        val_loader_multi_clf,\n",
    "        criterion, device\n",
    "    )\n",
    "    history_vgg16_aug[\"train_loss\"].append(train_loss)\n",
    "    history_vgg16_aug[\"train_acc\"].append(train_acc)\n",
    "    history_vgg16_aug[\"val_loss\"].append(val_loss)\n",
    "    history_vgg16_aug[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.3f} | \"\n",
    "          f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.3f}\")\n",
    "\n",
    "# Final eval, after training\n",
    "val_loss, val_acc, last_y_true, last_y_prob = evaluate(\n",
    "        vgg16_aug,\n",
    "        val_loader_multi_clf,\n",
    "        criterion, device\n",
    "    )\n",
    "y_true, y_prob = last_y_true, last_y_prob\n",
    "metrics_vgg16_aug = classification_metrics(y_true,y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35baab58",
   "metadata": {},
   "source": [
    "##### Save model weights, training info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869b5562",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_and_results(\n",
    "    save_dir=\"vgg16_aug\",\n",
    "    model=vgg16_aug,\n",
    "    history=history_vgg16_aug,\n",
    "    y_true=y_true,\n",
    "    y_prob=y_prob,\n",
    "    metrics=metrics_vgg16_aug,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcfab59",
   "metadata": {},
   "source": [
    "##### Results (Losses & Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f8b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16_aug, history_vgg16_aug, y_true_vgg16_aug, y_prob_vgg16_aug, cm_vgg16_aug, metrics_vgg16_aug = load_model_and_results(\n",
    "    save_dir=\"vgg16_aug\",\n",
    "    model=lambda: vgg16_aug,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21454bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n -- Classification report --\")\n",
    "print(metrics_vgg16_aug[\"classification_report\"])\n",
    "print(\"ROC AUC:\", metrics_vgg16_aug[\"roc_auc\"])\n",
    "print(\"AP (PR AUC):\", metrics_vgg16_aug[\"average_precision\"])\n",
    "\n",
    "plot_train_val_loss(history_vgg16_aug)\n",
    "plot_accuracy(history_vgg16_aug)\n",
    "plot_confusion_matrix(cm_vgg16_aug, multiclass_class_names)\n",
    "plot_roc_curve(y_true_vgg16_aug, y_prob_vgg16_aug)\n",
    "plot_pr_curve(y_true_vgg16_aug, y_prob_vgg16_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb508fcc",
   "metadata": {},
   "source": [
    "##### Results (Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ffac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the val set\n",
    "show_random_classif_predictions(model_vgg16_aug,val_multi_clf, device, class_names=multiclass_target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5060cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_pairs = study_most_confused_breeds(cm = cm_vgg16_aug, label_map = label_map_breed, top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13cdfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label_idx, pred_label_idx, _ = confusion_pairs[0] \n",
    "\n",
    "true_idx = label_map_breed.index(true_label_idx)  \n",
    "pred_idx = label_map_breed.index(pred_label_idx)\n",
    "\n",
    "# Plots examples of misclassified images\n",
    "plot_misclassified_pair(model_vgg16_aug, confusion_pairs, true_idx, pred_idx,\n",
    "                        label_map_breed, train_transform, device=device, n_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81173fc4",
   "metadata": {},
   "source": [
    "### 3.5 Conclusion on multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78850e7",
   "metadata": {},
   "source": [
    "| Task | Criterion | Optimizer | LR Scheduler | Model | Augmentation | Epochs | Train Loss | Val Loss | Train Acc | Val Acc | ROC AUC | PR AUC (AP) | Precision Cat | Recall Cat | Precision Dog | Recall Dog | F1-Score Cat | F1-Score Dog |Macro Precision | Macro Recall | Macro F1 |\n",
    "|------|-----------|-----------|--------|-----------|-----------|--------------|--------|------------|----------|-----------|----------|----------|--------------|----------------|-------------|----------------|-------------|---------------|----------------| ----------------| ----------------|\n",
    "| Multiclass Clf | CrossEntropyLoss | Adam (lr=1e-4, wd=1e-4) | No | vgg_16 | None |  | |  |  |  |  |  |  |  |  | | |  |  |  |  |\n",
    "| Multiclass Clf | CrossEntropyLoss | Adam (lr=1e-4, wd=1e-4) | No | vgg_16 | Yes | 20 |  | |  | |  |  |  |  |  |  |  |  | NA | NA | NA |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de888cd9",
   "metadata": {},
   "source": [
    "## 4 - Segmentation \n",
    "\n",
    "Note : Attention : certaines images n'ont pas de masques de segmentation, les retirer avant l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3988e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images sans masques : \n",
    "#show_image_mask_bbox(\"Egyptian_Mau_20\")\n",
    "#show_image_mask_bbox(\"Egyptian_Mau_162\")\n",
    "#show_image_mask_bbox(\"Egyptian_Mau_165\")\n",
    "#show_image_mask_bbox(\"Egyptian_Mau_196\")\n",
    "\n",
    "#show_image_mask_bbox(\"leonberger_18\")\n",
    "#show_image_mask_bbox(\"miniature_pinscher_14\")\n",
    "#show_image_mask_bbox(\"japanese_chin_199\")\n",
    "#show_image_mask_bbox(\"keeshond_7\")\n",
    "#show_image_mask_bbox(\"Persian_259\")\n",
    "#show_image_mask_bbox(\"wheaten_terrier_195\")\n",
    "\n",
    "#show_image_mask_bbox(\"saint_bernard_15\")\n",
    "#show_image_mask_bbox(\"saint_bernard_60\")\n",
    "#show_image_mask_bbox(\"saint_bernard_78\")\n",
    "#show_image_mask_bbox(\"saint_bernard_108\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f23bab7",
   "metadata": {},
   "source": [
    "## 5 - Comparative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed4cf97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02f81ffe",
   "metadata": {},
   "source": [
    "## Conclusion on project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d750fb3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
